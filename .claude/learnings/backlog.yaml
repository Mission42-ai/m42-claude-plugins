version: 1
extracted-from: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/.claude/sprints/2026-02-01_extract-command-refactor/transcriptions/phase-1_step-2_execute.log
extracted-at: '2026-02-01T19:15:00Z'
reviewed-at: '2026-01-19T10:48:00Z'
reviewed-by: ralph-iteration-22
learnings:
  - id: handle-large-files-with-read-tool
    status: applied
    applied-at: '2026-01-19T10:50:00Z'
    applied-in-iteration: 23
    title: Handle large files with Read tool using offset/limit or alternatives
    problem: 'Reading large files without offset/limit parameters causes token limit errors:

      - Files >25k tokens: "File content exceeds maximum allowed tokens (25000)"

      - Files >40k tokens: Requires more conservative approach


      This occurred across multiple iterations with files like:

      - sprint-loop.sh (25-26k tokens)

      - page.ts (40k tokens)

'
    solution: "When encountering large files, use these strategies:\n\n1. Use offset and limit parameters to read specific sections:\n   - offset: 1, limit: 200 (read from beginning)\n   - offset: 690, limit: 150 (read middle section)\n   - For very large files (>40k), start with limit: 500 or less\n\n2. Use Grep tool instead for searching specific content\n\n3. Read smaller, related files first (e.g., read implement-feature.md instead of sprint-loop.sh when exploring patterns)\n\n4. If you only need specific sections, use offset to skip to relevant parts\n"
    target: /home/konstantin/projects/CLAUDE.md
    confidence: high
    source:
      tool: Read
      command: Multiple large file reads without offset/limit
      error: File content exceeds maximum allowed tokens
      instances: 5
  - id: fix-read-file-path-for-status-server
    status: rejected
    title: Fix Read file path for status server
    problem: 'Attempted to read non-existent file: routes.ts

      Error: "<tool_use_error>File does not exist.</tool_use_error>"


      Failed file path: /home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/routes.ts

'
    solution: 'Corrected to existing file: worktree.ts


      Success file path: /home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/worktree.ts


      When working with status server files, verify file names match the actual codebase structure.

'
    target: home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/CLAUDE.md
    confidence: high
    source:
      tool: Read
      command: file_path=/home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/routes.ts
      error: <tool_use_error>File does not exist.</tool_use_error>
    rejection-reason: Too specific - one-time filename mistake, not a generalizable pattern
  - id: handle-read-tool-large-file-with-offset-limit
    status: rejected
    title: Handle Read tool large file with offset/limit
    problem: 'Attempted to read a large file without using offset/limit parameters.

      File: /home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/scripts/sprint-loop.sh (26016 tokens)

      Error: "File content (26016 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content."

'
    solution: 'Instead of reading the entire large file, the agent successfully switched to reading a smaller, more specific file.

      Successful approach: Read /home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/patterns/implement-feature.md instead.


      When encountering large files:

      1. Use offset and limit parameters to read specific portions

      2. Consider using Grep to search for specific content

      3. If exploring patterns/configs, read smaller related files first

'
    target: home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Read
      command: file_path=/home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/scripts/sprint-loop.sh
      error: File content (26016 tokens) exceeds maximum allowed tokens (25000)
    rejection-reason: Consolidated into handle-large-files-with-read-tool
  - id: handle-read-tool-large-file-with-limit
    status: rejected
    title: Handle Read tool large file with limit
    problem: 'Attempted to read a very large file (page.ts, 40540 tokens) without using offset/limit parameters.

      File: /home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/page.ts

      Error: "File content (40540 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content."

'
    solution: 'Successfully added limit parameter to read only the first portion of the file.

      Successful approach: Added limit: 500 to the Read tool call.


      When encountering large files:

      1. Use limit parameter to read a reasonable number of lines (e.g., 500)

      2. Use offset and limit together to read specific portions

      3. Consider using Grep to search for specific content instead of reading entire file

      4. For very large files (>40k tokens), start with a small limit to avoid token overflow

'
    target: /home/konstantin/projects/CLAUDE.md
    confidence: medium
    source:
      tool: Read
      command: file_path=/home/konstantin/projects/m42-claude-plugins/plugins/m42-sprint/compiler/src/status-server/page.ts
      error: File content (40540 tokens) exceeds maximum allowed tokens (25000)
    rejection-reason: Consolidated into handle-large-files-with-read-tool
  - id: preflight-context-verification
    status: applied
    title: Verify context files before declaring preflight complete
    problem: 'During sprint preflight, context files may already exist from previous runs.

      Simply checking git status isn''t sufficient - need to verify the content

      matches what''s needed for the sprint.

'
    solution: 'In preflight phases:

      1. Check if context directory exists

      2. Read existing context files (_shared-context.md, implementation-plan.md)

      3. Verify they contain required information

      4. Check git log for preflight commit

      5. Only re-create if missing or incomplete


      This prevents duplicate work and ensures context quality.

'
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Read, Bash
      context: Phase-0 preflight verified existing context before completing
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: sprint-worktree-context-paths
    status: applied
    title: Use worktree-specific paths when referencing sprint context
    problem: 'Sprint context files live in the worktree-specific directory structure:

      trees/<sprint-id>/.claude/sprints/<sprint-id>/context/


      When reading or creating context, agents must use the worktree path,

      not the main repo path.

'
    solution: 'Always use full worktree paths for sprint context:

      - Context: trees/<sprint-id>/.claude/sprints/<sprint-id>/context/

      - Transcripts: trees/<sprint-id>/.claude/sprints/<sprint-id>/transcriptions/


      The sprint framework handles this via CWD, but explicit paths prevent confusion.

'
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: All file reads used full worktree paths consistently
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: plugin-structure-discovery-sequence
    status: applied
    title: Follow systematic discovery sequence for plugin codebases
    problem: 'When researching a plugin''s architecture, random file exploration

      wastes time and misses key relationships between components.

'
    solution: 'Use this discovery sequence for plugins:

      1. Read root package.json for build/test commands

      2. Read plugin.json for command/skill/agent registry

      3. Read main README.md for architecture overview

      4. Read specific command .md files for workflows

      5. Read subagent .md files for delegation patterns

      6. Read skill SKILL.md files for domain knowledge


      This follows the dependency chain: high-level → implementation → domain.

'
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Task (Explore subagent)
      context: Systematic codebase exploration in lines 6-72
    applied-at: '2026-02-01T18:23:51Z'
    applied-by: m42-signs:extract --auto-apply-high
  - id: task-subagent-for-broad-exploration
    status: applied
    title: Delegate broad codebase exploration to Task(Explore) subagent
    problem: 'When a task requires understanding multiple directories, file types,

      and architectural patterns, doing this inline bloats the main conversation

      and loses focus.

'
    solution: 'Use Task tool with subagent_type="Explore" for:

      - "Understand project structure"

      - "Find all X in the codebase"

      - "How does Y work?"

      - Initial research before creating artifacts


      The Explore agent returns a focused summary that informs next steps

      without polluting context with dozens of file reads.

'
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Task
      context: Line 6 delegated codebase research to Explore subagent
    applied-at: '2026-02-01T18:23:51Z'
    applied-by: m42-signs:extract --auto-apply-high
  - id: preflight-commit-convention
    status: applied
    title: Use "preflight:" prefix for sprint context setup commits
    problem: 'Sprint phases need to identify when preflight is complete via git log.

      Generic commit messages make this hard to detect programmatically.

'
    solution: 'Preflight commits must use format:

      "preflight: sprint context prepared"


      This allows:

      - `git log --oneline -1 --grep="^preflight:"` to check completion

      - Clear separation between setup and development commits

      - Consistent pattern across all sprints

'
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Bash (git log)
      context: Lines 87, 97 checked for "preflight:" commit
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: test-directory-structure
    status: applied
    title: Project uses top-level tests/ directory for integration tests
    problem: 'When looking for test patterns or adding new tests, need to know

      where integration tests live vs unit tests.

'
    solution: 'Test structure:

      - `tests/` (root) - Integration tests that span plugins

      - `plugins/*/tests/` - Plugin-specific unit tests (if present)

      - `plugins/*/e2e/` - End-to-end tests for specific plugins


      Integration tests use shell scripts (.sh) and test cross-plugin workflows.

'
    target: ./CLAUDE.md
    confidence: medium
    source:
      tool: Bash, Read
      context: Lines 34, 50, 56 explored test structure
    applied-at: '2026-02-01T18:23:51Z'
    applied-by: m42-signs:extract --auto-apply-high
  - id: delegate-research-to-explore-preflight
    status: applied
    title: Delegate codebase research to Explore subagent during preflight
    problem: |
      Sprint preflight phases need to understand project structure, test framework, patterns, and key files before implementation begins. Doing this inline can be inefficient and scattered.
    solution: |
      Use Task(subagent_type="Explore") to delegate systematic codebase research. The subagent will methodically explore project structure, identify patterns, locate key files, and return a comprehensive summary. This creates reusable context documents (_shared-context.md and implementation-plan.md) that all subsequent phases can reference.
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Task
      context: Preflight phase spawned Explore subagent with structured research prompt covering project structure, test framework, and key patterns
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: bash-scenario-test-pattern
    status: applied
    title: Test framework uses Bash scenario pattern with SCORE/TOTAL
    problem: |
      Need to understand the project's testing approach to create compatible tests for new components.
    solution: |
      This project uses Bash scenario tests (located in tests/*.sh) with a SCORE/TOTAL scoring pattern. Each test checks multiple scenarios, increments SCORE on pass, and exits 0 if SCORE == TOTAL. Tests include file existence, frontmatter validation, and required content checks.
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: Test files show set -euo pipefail, SCORE=0, TOTAL=6, conditional SCORE increment
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-frontmatter-structure
    status: applied
    title: Subagent frontmatter requires name, description, tools, model, color
    problem: |
      Creating subagents requires knowing the exact frontmatter structure and required fields to ensure they load correctly.
    solution: |
      Subagent files must have YAML frontmatter with exactly these fields: name (kebab-case identifier), description (when/why to invoke), tools (comma-separated list like "Read, Bash, Skill"), model (sonnet/haiku/inherit), color (cyan/purple/blue/green for visual grouping). The frontmatter is delimited by --- markers.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: chunk-analyzer.md and _shared-context.md document the exact frontmatter pattern
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: operator-pattern-domain-logic-separation
    status: applied
    title: Operator pattern separates orchestration from domain logic via skills
    problem: |
      Commands can become bloated (400+ lines) when they mix orchestration logic with domain knowledge like taxonomies, criteria, and patterns.
    solution: |
      Use the operator pattern - commands contain only argument parsing, preflight checks, Task() orchestration, and output aggregation. All domain logic moves to skills that subagents invoke with Skill(command='skill-name'). This enables parallel subagent processing while keeping domain knowledge centralized and reusable.
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: implementation-plan.md documents transformation from 400-line monolith to ~150-line operator
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-color-semantic-coding
    status: pending
    title: Use color coding to signal subagent purpose in operator workflows
    problem: |
      When orchestrating multiple subagents, it's helpful to visually distinguish their roles for debugging and understanding workflow flow.
    solution: |
      Assign semantic colors to subagents based on their function: cyan for research/analysis, purple for review/audit, blue for implementation, green for testing. This creates visual grouping in logs and makes the workflow architecture immediately apparent.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: Read
      context: _shared-context.md documents explicit color mappings for different subagent roles
  - id: skill-frontmatter-structure
    status: applied
    title: Skills require SKILL.md frontmatter with name and description
    problem: |
      Need to know the required structure for creating skills that will be recognized and loadable by the plugin system.
    solution: |
      Skills must have a SKILL.md file in skills/<skill-name>/ with YAML frontmatter containing at minimum: name (skill identifier) and description (when to invoke, what it provides). Supporting materials go in a references/ subdirectory.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: managing-signs/SKILL.md shows canonical structure with frontmatter and references/
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: sprint-context-files-preflight
    status: applied
    title: Create _shared-context.md and implementation-plan.md during preflight
    problem: |
      Sprint steps need consistent access to project patterns, file paths, schemas, and the overall implementation strategy.
    solution: |
      During preflight, create two standardized context files: _shared-context.md (project info, test framework, key paths, patterns, schemas, edge cases) and implementation-plan.md (goals, current problems, architecture, files to create/modify, phases, verification). These become reference documentation for all subsequent sprint phases.
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Write
      context: Preflight successfully created both context files that documented comprehensive sprint context
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: edge-case-handling-matrix
    status: applied
    title: Document edge case handling matrix in sprint context
    problem: |
      Complex commands need to handle edge cases gracefully, and all sprint phases should handle them consistently.
    solution: |
      Create an "Edge Cases to Handle" matrix in _shared-context.md mapping each edge case to its expected response. Include cases like empty input, no content, mechanical tasks, all-filtered results, and duplicates. This ensures consistent handling across all implementation phases.
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: _shared-context.md contains table with 5 edge cases mapped to expected responses
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: skill-reference-sharing
    status: pending
    title: Skills can share common references instead of duplicating
    problem: |
      Multiple skills may need the same reference documentation (schemas, formats), leading to duplication and sync issues.
    solution: |
      Reference existing materials from other skills where applicable rather than duplicating. For example, learning-extraction skill should reference transcript-format.md and backlog-schema.md from managing-signs/references/ rather than creating copies. Consider moving truly shared references to a common location.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: Read
      context: implementation-plan.md explicitly notes to reuse existing references from managing-signs
  - id: extract-token-vs-line-count-threshold
    status: applied
    title: Current extract command uses line count instead of token count for thresholds
    problem: |
      The existing extract.md uses file size and line counts to determine if preprocessing is needed, but Claude's actual limits are token-based. This causes incorrect threshold decisions.
    solution: |
      When refactoring, use token-based thresholds or attempt direct read with fallback to preprocessing on token errors. The implementation plan explicitly notes this as a "Current Problem: Broken thresholds - Uses line count/file size instead of token count".
    target: plugins/m42-signs/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: implementation-plan.md lists broken thresholds as current problem with solution
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: sprint-creation-before-refactoring
    status: applied
    title: Structure sprint implementation as creation phases before refactoring
    problem: |
      Refactoring existing code while simultaneously creating new dependencies can lead to broken intermediate states.
    solution: |
      Order sprint phases so all new components (skills, subagents) are created and tested individually before refactoring the existing command that depends on them. This ensures the refactored command can immediately use validated components.
    target: plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: SPRINT.yaml shows dependency ordering with creation phases before refactor-command
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: learning-taxonomy-eight-categories
    status: applied
    title: Eight-category learning taxonomy for transcript extraction
    problem: |
      Need a structured taxonomy to categorize different types of learnings extracted from transcripts.
    solution: |
      Use an 8-category taxonomy: Architectural Patterns (component relationships, design decisions), Project Conventions (naming, file organization, code style), Pitfalls & Gotchas (failures, edge cases), Effective Strategies (what worked, debugging techniques), File Relationships (dependencies, files that change together), API & Library Patterns (correct API usage), Build & Test Patterns (commands, organization), Domain Knowledge (business logic, terminology, constraints).
    target: plugins/m42-signs/CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: SPRINT.yaml and implementation-plan.md specify the 8-category taxonomy
    applied-at: "2026-02-01T18:26:29Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-artifact-validation-checklist
    status: applied
    title: Use validation checklist to determine if artifact should be subagent
    problem: |
      When creating artifacts with /m42-meta-toolkit:create-* commands, determining if they should be a subagent vs command vs skill is not always obvious from the description alone.
    solution: |
      Use this validation checklist for subagents: 1) Autonomous sub-task? (operates independently), 2) Dedicated scope? (focused single responsibility), 3) Separate domain? (invokes Skill() for specialized knowledge), 4) Invoked by parent? (called via Task() by commands or other subagents). If yes to all four, it's correctly a subagent.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: assistant reasoning
      context: Step 2 validation in transcript shows systematic checklist application
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-operator-pattern
    status: applied
    title: Design subagents to orchestrate not educate
    problem: |
      Subagents can become bloated with embedded domain knowledge (taxonomies, schemas, patterns), making them hard to maintain and reducing conciseness.
    solution: |
      Subagents should orchestrate process flow (50-200 words) and delegate all specialized knowledge to skills via Skill(command='skill-name'). The subagent prompt describes WHAT to do and WHEN to invoke skills. The skill provides HOW (domain knowledge, reference materials, examples).
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: assistant reasoning
      context: Creating-subagents skill guidance and draft implementation in transcript
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: artifact-quality-review-workflow
    status: applied
    title: Use independent quality review with iteration for artifacts
    problem: |
      Created artifacts (subagents, skills, commands) may have quality issues that aren't caught until they fail in production or cause confusion.
    solution: |
      After drafting any artifact, invoke Task(subagent_type='artifact-quality-reviewer') with artifact path and description. The reviewer returns structured scores (1-5 scale) across multiple categories and actionable improvements. Iterate on feedback until all scores ≥4/5. Mark review scores in final documentation.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task
      context: Step 5 shows artifact-quality-reviewer returning 5/5 scores with actionable feedback
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-minimal-tool-selection
    status: applied
    title: Grant subagents only minimal necessary tools
    problem: |
      Subagents are often granted too many tools unnecessarily, which can lead to unexpected behavior or security concerns.
    solution: |
      Grant only minimal necessary tools. If a subagent delegates all work via Skill(), it only needs Read and Skill tools, not Bash or other execution tools. Review tool list and remove any tools the subagent doesn't directly invoke in its workflow.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality review feedback)
      context: Reviewer suggested removing Bash from tools since subagent only uses Read and Skill()
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-model-inheritance
    status: applied
    title: Use model inheritance unless specific capabilities required
    problem: |
      Hardcoding specific models (e.g., model: sonnet) in subagent frontmatter can be inefficient when parent context already uses an appropriate model.
    solution: |
      Use 'model: inherit' to match parent context unless the subagent requires specific model capabilities (e.g., haiku for speed, opus for complex reasoning). This optimizes cost and maintains consistency with the calling context.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality review feedback)
      context: Reviewer suggested changing model:sonnet to model:inherit
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-directive-opening
    status: applied
    title: Start subagent prompts with directive action not headings
    problem: |
      Subagent prompts that start with headings (## Step 1) or explanatory text waste tokens and reduce clarity.
    solution: |
      Make the opening directive and action-oriented. Remove headings. Structure: [Role: 1 sentence] → [Core instructions: 3-5 directive statements] → [Constraints if needed] → [Skill references]. Example: "Analyze transcript section for learning extraction." not "## Analysis Process\n\nYou will analyze..."
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality review feedback)
      context: Reviewer suggested removing heading and making opening more directive
    applied-at: "2026-02-01T18:53:00Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-color-coding-purpose
    status: pending
    title: Use color coding to reflect subagent primary purpose
    problem: |
      Subagent colors should help visually organize workflows and indicate the agent's role, but arbitrary color choices reduce this benefit.
    solution: |
      Assign colors based on primary purpose: cyan (research/analysis), blue (implementation), green (testing/validation), purple (review/audit), yellow (documentation), orange (maintenance), red (debugging), magenta (deployment). Document the color system in references/color-codes.md for consistency.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: assistant reasoning
      context: Transcript shows cyan chosen for analysis subagent, mentions color system
  - id: skill-reference-frontmatter-standardization
    status: applied
    title: Use standardized frontmatter for skill reference files
    problem: |
      Reference files within a skill directory need consistent metadata to support documentation discovery, search, and skill association tracking.
    solution: |
      All reference files in skills/*/references/*.md should include YAML frontmatter with these fields:
      - title: Descriptive title of the reference
      - description: What the reference covers
      - keywords: Searchable terms (comma-separated)
      - file-type: Always "reference" for reference files
      - skill: Parent skill name (matches the skill directory name)

      Example:
      ```yaml
      ---
      title: Learning Taxonomy - 8 Categories
      description: Category definitions for classifying extracted learnings
      keywords: learning categories, taxonomy, classification
      file-type: reference
      skill: learning-extraction
      ---
      ```
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Write
      context: Created 4 reference files with identical frontmatter pattern matching managing-signs references
    applied-at: "2026-02-01T18:38:47Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: skill-two-tier-structure
    status: applied
    title: Organize skills with SKILL.md + references/ subdirectory pattern
    problem: |
      Skills with extensive domain knowledge (>200 lines) become difficult to navigate if all content is in a single SKILL.md file. Subagents need both high-level guidance and detailed reference materials.
    solution: |
      Use this two-tier structure for complex skills:
      ```
      skills/skill-name/
      ├── SKILL.md              # Core concepts, taxonomy tables, workflow overview
      └── references/           # Detailed specifications and examples
          ├── concept-1.md
          ├── concept-2.md
          └── concept-3.md
      ```

      SKILL.md should:
      - Provide overview and taxonomy/categories
      - Reference detailed docs: "See references/X.md for detailed Y"
      - Include 1-2 complete examples demonstrating the workflow

      Reference files should:
      - Deep-dive into one specific aspect
      - Include extensive examples (good vs bad)
      - Provide edge case handling
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Write, Read
      context: Both managing-signs and learning-extraction skills follow this pattern
    applied-at: "2026-02-01T18:38:47Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: subagent-skill-invocation-pattern
    status: applied
    title: Subagents invoke skills with Skill() tool not Read
    problem: |
      Subagents need to load domain knowledge from skills, but the mechanism for doing so must be explicit and consistent.
    solution: |
      In subagent prompts (agents/*.md), use this pattern to load skill knowledge:

      ```markdown
      ## Process

      Invoke Skill(command='skill-name') to load domain knowledge.

      [Then describe what to do with that knowledge]
      ```

      Do NOT instruct subagents to Read the SKILL.md file directly. The Skill() tool handles skill loading and trigger matching.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Write
      context: transcript-section-analyzer.md uses Skill(command='learning-extraction') pattern
    applied-at: "2026-02-01T18:38:47Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: delegate-skill-creation-to-creating-skills
    status: applied
    title: Delegate skill creation to creating-skills skill via Skill() tool
    problem: |
      Creating skills requires understanding skill structure, frontmatter requirements, reference organization, and validation patterns. Implementing this manually risks inconsistency.
    solution: |
      When asked to create a skill, invoke the creating-skills skill:

      ```
      Skill(skill="m42-meta-toolkit:creating-skills", args="Create [skill-name] skill at [path]

      [Describe the domain knowledge needed and reference files]")
      ```

      The creating-skills skill handles:
      - Skill vs command vs subagent validation
      - SKILL.md structure and frontmatter
      - Reference file organization and frontmatter
      - Consistent formatting and examples
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Skill
      context: Agent invoked creating-skills skill which correctly created SKILL.md + 4 references
    applied-at: "2026-02-01T18:38:47Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: operator-pattern-skill-extraction
    status: applied
    title: Extract domain logic from commands into skills for subagent reuse
    problem: |
      Commands mixing orchestration (argument parsing, Task() calls, result aggregation) with domain knowledge (taxonomies, quality criteria, extraction patterns) become bloated (400+ lines) and prevent domain knowledge reuse across subagents.
    solution: |
      Apply the operator pattern:
      - **Commands**: Argument parsing, preflight checks, Task() orchestration, output aggregation only
      - **Skills**: All domain knowledge, taxonomies, criteria, patterns, scoring rubrics
      - **Subagents**: Invoke skills via Skill() tool to load domain knowledge, then execute specific analysis tasks

      This enables:
      - Parallel subagent processing (each loads the same skill)
      - Domain knowledge reuse across multiple subagents
      - Smaller, focused command files (~150 lines)
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Read
      context: Implementation plan shows refactor from 400-line monolith to operator pattern
    applied-at: "2026-02-01T18:38:47Z"
    applied-by: m42-signs:extract --auto-apply-high
  - id: write-requires-read-first
    status: pending
    title: Write tool requires Read before overwriting existing files
    problem: |
      When using Write tool to overwrite an existing file, forgetting to Read it first causes an error: "File has not been read yet. Read it first before writing to it."
    solution: |
      Always Read the file first before calling Write to overwrite it. The workflow should be:
      1. Glob or check if file exists
      2. Read the existing file to load its content into context
      3. Write the new content to overwrite it
      
      This is a safety mechanism to prevent accidental overwrites without seeing current content.
    target: ./CLAUDE.md
    confidence: high
    source:
      tool: Write
      context: Transcript shows error when attempting Write without prior Read
  - id: creating-subagents-workflow-seven-steps
    status: pending
    title: Follow seven-step workflow when creating subagents via create-subagent command
    problem: |
      Subagent creation requires systematic approach to ensure quality and consistency. Ad-hoc creation leads to missing frontmatter, poor tool selection, or bloated prompts.
    solution: |
      Use this 7-step workflow (from m42-meta-toolkit:create-subagent):
      1. Analyze Description - Extract name, location, project vs global
      2. Validate Artifact Type - Confirm it should be a subagent using 4-point checklist
      3. Read Implementation Plan - Load context from sprint/project docs
      4. Draft Subagent - Invoke creating-subagents skill to guide creation
      5. Independent Review - Task(artifact-quality-reviewer) for quality assessment
      6. Iterate on Feedback - Address all Major issues, aim for 4+/5 scores
      7. Write Final Subagent - Write quality-reviewed version to target path
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: TodoWrite, Task
      context: Transcript shows explicit 7-step workflow with task tracking
  - id: iterative-quality-review-until-approval
    status: pending
    title: Iterate on quality review feedback until all scores reach 4+/5
    problem: |
      First drafts of artifacts often have quality issues. Accepting a draft with low scores (2/5, 3/5) leads to poor quality artifacts that confuse future agents or fail in practice.
    solution: |
      After artifact-quality-reviewer returns feedback:
      1. Address ALL "Major" issues immediately
      2. Fix "Minor" issues if feasible
      3. Re-run quality review to verify improvements
      4. Repeat until all category scores >= 4/5
      5. Aim for "APPROVE" recommendation before finalizing
      
      The transcript shows improvement from mixed scores to 5/5 across all categories after one iteration.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task
      context: Two quality review cycles - first identified issues, second confirmed 5/5 scores
  - id: subagent-imperative-form-prompts
    status: pending
    title: Write subagent prompts in imperative form not second-person
    problem: |
      Using second-person ("You will receive", "Your workflow") in subagent prompts wastes tokens and reduces clarity. Subagents execute instructions, they don't need conversational framing.
    solution: |
      Use imperative form throughout subagent prompts:
      - "Receives:" instead of "You will receive"
      - "Workflow:" instead of "Your workflow is"
      - "Loads domain knowledge" instead of "You will load domain knowledge"
      - "Extracts patterns" instead of "You extract patterns"
      
      This creates more directive, concise prompts (target: 50-200 words for orchestration subagents).
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality review feedback)
      context: Reviewer flagged second-person usage as "Major" issue, iteration fixed it
  - id: subagent-proactive-trigger-patterns
    status: pending
    title: Include proactive trigger patterns in subagent descriptions
    problem: |
      Subagent descriptions that only explain what the subagent does don't help the orchestrating agent know WHEN to invoke it proactively.
    solution: |
      Subagent description should include two parts:
      1. **What it does**: Core function (e.g., "Analyzes transcript sections")
      2. **When to use**: Proactive triggers (e.g., "Use proactively when extract command delegates section analysis for parallel processing")
      
      This enables parent commands/agents to discover and invoke subagents appropriately.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality review feedback)
      context: Invocation Patterns scored 2/5, improved to 5/5 after adding trigger patterns
  - id: validation-script-path-issues
    status: pending
    title: Plugin validation scripts may fail in worktree contexts
    problem: |
      Plugin validation scripts expect to run from the main repo, not from sprint worktrees. Scripts like validate_subagent.py fail with "No such file or directory" when invoked from trees/* worktrees.
    solution: |
      When working in sprint worktrees (trees/sprint-id/), validation commands may fail because:
      1. ~/.claude/plugins/cache/ paths point to main repo structure
      2. Scripts aren't available in worktree context
      
      For now, skip validation in worktrees or run validation from main repo after sprint completion. Future improvement: Make validation scripts worktree-aware.
    target: plugins/m42-sprint/CLAUDE.md
    confidence: medium
    source:
      tool: Bash
      context: Multiple validation script failures with path errors in worktree
  - id: python-vs-python3-command
    status: pending
    title: Use python3 not python in validation scripts
    problem: |
      Systems may not have 'python' command available, only 'python3'. Scripts using 'python' shebang or command fail with "command not found".
    solution: |
      Always use 'python3' for Python script invocation:
      - Shebangs: #!/usr/bin/env python3
      - Direct calls: python3 script.py
      - Bash commands: python3 -m module
      
      This ensures compatibility across Ubuntu/Debian systems where 'python' is not aliased.
    target: ./CLAUDE.md
    confidence: medium
    source:
      tool: Bash
      context: Validation attempt failed with "python: command not found"
- id: extract-refactor-1
  status: pending
  title: Skills use gerund naming, subagents use imperative naming
  problem: Inconsistent naming across artifact types makes codebase harder to navigate and understand artifact purpose from
    name alone.
  solution: Follow naming conventions - Skills use gerund form (learning-extraction, managing-signs, creating-skills), Subagents
    use imperative form (analyze-transcript.md, match-context.md), Commands use imperative without .md extension (extract,
    review, apply). This reflects that skills are domain knowledge (ongoing capability), subagents are actors (perform action),
    commands are user-facing operations.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 1 from transcript extraction (Project Conventions)
- id: extract-refactor-2
  status: pending
  title: SKILL.md uses imperative/infinitive form not second person
  problem: Inconsistent writing style in skills makes them harder for AI agents to parse and creates confusion about whether
    instructions are directives or descriptions.
  solution: Write entire SKILL.md using imperative/infinitive form (verb-first instructions) with objective, instructional
    language. Use "To accomplish X, do Y" rather than "Do X" or "You should do X". This maintains consistency and clarity
    for AI consumption while avoiding ambiguity about applicability.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 2 from transcript extraction (Project Conventions)
- id: extract-refactor-3
  status: pending
  title: Subagent parallel processing via Task tool for large transcript extraction
  problem: Processing large transcripts (100+ blocks) sequentially is slow and doesn't utilize Claude's capability to spawn
    multiple subagent tasks concurrently.
  solution: Implement parallel processing workflow - split transcript into sections (~50 blocks each), spawn Task(transcript-section-analyzer)
    for each section in parallel, collect all candidate learnings, then run sequential aggregation phases (context-matching,
    quality-review). Commands should support `--parallel` flag to enable this mode. Operator pattern (command as orchestrator,
    subagents as workers) enables this architecture.
  target: plugins/m42-signs/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 3 from transcript extraction (Architectural Patterns)
- id: extract-refactor-4
  status: pending
  title: TodoWrite tool tracks task decomposition and progress
  problem: Complex multi-step workflows lose track of progress, making it hard to know what's completed, what's in-progress,
    and what remains.
  solution: Use TodoWrite tool at workflow start to create task list, update status as work progresses (pending → in_progress
    → completed). Structure shows both passive form ("Analyze description...") and active form ("Analyzing description...")
    for clarity. Tool returns confirmation and current state, helping maintain workflow context across long sessions.
  target: ./CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 4 from transcript extraction (Effective Strategies)
- id: extract-refactor-5
  status: pending
  title: Command refactoring reduces size from 400 to 150 lines using skills
  problem: The extract command mixed orchestration (argument parsing, Task calls, output formatting) with domain knowledge
    (taxonomy, quality criteria, patterns) in a single 400-line file, making it hard to maintain and preventing knowledge
    reuse.
  solution: Refactor to operator pattern - retain ~150 lines for orchestration (args, preflight, Task orchestration, aggregation,
    output), move ~250 lines of domain knowledge to learning-extraction skill with reference files. This achieves ~63% size
    reduction while improving maintainability and enabling knowledge reuse across multiple commands and subagents.
  target: plugins/m42-signs/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 5 from transcript extraction (Architectural Patterns)
- id: extract-refactor-6
  status: pending
  title: Package_skill.py validates before packaging
  problem: Packaging skills with errors wastes time discovering issues after distribution, when they manifest as runtime failures
    or poor performance.
  solution: The package_skill.py script automatically validates skills before creating zip file - checks YAML frontmatter
    format, required fields, naming conventions, directory structure, description quality, file organization, resource references.
    If validation fails, script reports errors and exits without packaging. Fix errors and rerun packaging.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 6 from transcript extraction (Build & Test Patterns)
- id: extract-refactor-7
  status: pending
  title: Skill invocation via Skill tool loads domain knowledge into subagent context
  problem: Subagents need access to domain-specific knowledge (taxonomies, patterns, criteria) to perform specialized analysis,
    but embedding this in subagent prompts causes bloat and duplication.
  solution: Design subagents to invoke Skill() tool with skill name - this loads the skill's SKILL.md and on-demand references
    into subagent context. Subagent prompts should be thin orchestration ("analyze this section using learning-extraction
    skill"), delegating domain knowledge to the skill. The `description` field's trigger patterns control automatic loading.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 7 from transcript extraction (API & Library Patterns)
- id: extract-refactor-8
  status: pending
  title: Malformed JSON lines should be skipped during transcript processing
  problem: Real-world transcripts may contain malformed JSON lines due to interruptions, logging errors, or partial writes.
    Strict parsing causes extraction to fail completely.
  solution: Design transcript processors to skip malformed lines gracefully. Document expected malformed line count (e.g.,
    "111 valid lines, 8 malformed lines should be skipped"). Use jq with --raw-output or line-by-line parsing with try-catch,
    counting valid vs. skipped lines. Report statistics showing lines analyzed vs. skipped.
  target: plugins/m42-signs/CLAUDE.md
  confidence: medium
  source:
    tool: extract
    context: Learning 8 from transcript extraction (Pitfalls & Gotchas)
- id: touch-command-bypasses-write-requirement
  status: pending
  title: Touch command bypasses Write tool file existence requirement
  problem: |
    Write tool requires files to exist and be read before writing. For brand new files, this creates a chicken-egg problem.
  solution: |
    Use `Bash(touch <filepath>)` to create empty file, optionally followed by `Read(<filepath>)` to load into context, then use Write tool. Alternatively, for files needing complete content from scratch, use `Bash(cat > <file> << 'EOF'...EOF')` heredoc pattern which bypasses the read requirement entirely.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Read (from CLAUDE.md learning)
    context: Documented pattern for creating new files in sprint worktree

- id: bash-heredoc-creates-files-without-read
  status: pending
  title: Bash with heredoc creates files without Read requirement
  problem: |
    For creating new files with complete content known upfront, Write tool's requirement to Read first is burdensome.
  solution: |
    Use Bash with heredoc pattern - `cat > <filepath> << 'EOF'...content...EOF` or `cat > <filepath> << 'DELIMITER'...content...DELIMITER`. Use single quotes around delimiter (`'EOF'`) to prevent shell expansion. This bypasses Write tool's read requirement and works in one step.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Read (from CLAUDE.md learning)
    context: Documented pattern for single-step file creation

- id: subagent-creation-workflow-systematic
  status: pending
  title: Follow systematic subagent creation workflow with quality review
  problem: |
    Creating subagents ad-hoc leads to missing frontmatter fields, incorrect tool selection, bloated prompts, or wrong artifact type choice.
  solution: |
    Use systematic workflow: 1) Analyze description and determine location, 2) Validate artifact type (subagent vs command vs skill), 3) Draft using creating-subagents skill, 4) Independent review via artifact-quality-reviewer, 5) Iterate until scores ≥4/5, 6) Write final version. The transcript shows this workflow completing successfully with quality scores improving from 4.83/5 to 5/5.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Task, TodoWrite
    context: Transcript demonstrates complete 6-step workflow with tracking and iteration

- id: subagent-quality-reviewer-pattern
  status: pending
  title: Quality reviewer subagents use purple color and minimal tools
  problem: |
    Review/audit subagents need consistent conventions for discoverability and appropriate tool access.
  solution: |
    Quality reviewer subagents should use: color=purple (review/audit category), model=sonnet (balanced capability), tools="Read, Skill" (read-only, safe), and strict filtering philosophy (prefer false negatives over false positives). They score candidates on multiple dimensions and output filtered YAML with rejection reasons and statistics.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Write
    context: quality-reviewer.md created with purple color, Read/Skill tools, strict filtering

- id: validation-script-worktree-path-issues
  status: pending
  title: Plugin validation scripts expect main repo paths not worktree paths
  problem: |
    Sprint worktrees (trees/*/) can't access validation scripts at ~/.claude/plugins/cache/ paths because those point to main repo structure.
  solution: |
    When running validation scripts from worktrees, use the worktree's copy of the script instead of the cache path. Pattern: `python3 /path/to/worktree/plugins/*/skills/*/scripts/validate_*.py` instead of `python3 ~/.claude/plugins/cache/.../scripts/validate_*.py`. The Glob tool can find the correct worktree path.
  target: plugins/m42-sprint/CLAUDE.md
  confidence: high
  source:
    tool: Bash, Glob
    context: Validation failed with cache path, succeeded after Glob found worktree path

- id: python3-not-python-command
  status: pending
  title: Use python3 command not python for validation scripts
  problem: |
    Many systems don't have 'python' command, only 'python3'. Using 'python' causes "command not found" errors.
  solution: |
    Always use 'python3' for script invocation: `python3 script.py` not `python script.py`. This ensures compatibility with Ubuntu/Debian systems where 'python' is not aliased to python3.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Bash
    context: First validation attempt failed with "python: command not found", fixed with python3

- id: subagent-validation-passes-with-warnings
  status: pending
  title: Subagent validation can pass with warnings (15/15 checks, 100%)
  problem: |
    Understanding the difference between passing validation with warnings vs. errors is important for iteration decisions.
  solution: |
    Validation script distinguishes: Errors (✗) block validation, Warnings (⚠) are advisory and don't block. A subagent can achieve 15/15 (100%) pass rate with warnings present. Warnings like "directive language" or "word count outside target" should be reviewed manually but don't prevent deployment if other quality criteria are met.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Bash (validation output)
    context: quality-reviewer draft passed 15/15 checks with 1 warning about directive language

- id: artifact-quality-reviewer-score-interpretation
  status: pending
  title: Artifact quality reviewer scores 1-5 with 4+ threshold for approval
  problem: |
    Need to understand when to iterate vs. approve during artifact creation workflows.
  solution: |
    artifact-quality-reviewer scores on 6 categories (Frontmatter, Prompt Quality, Tool Selection, Skill Integration, Invocation Patterns, Artifact Type) using 1-5 scale: 5=Excellent (90-100%), 4=Good (75-89%), 3=Acceptable (60-74%), 2=Needs Work (40-59%), 1=Poor (<40%). Average ≥4/5 is approval threshold. Address Major issues (categories <4), optionally fix Minor issues (categories 4).
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Read, Task
    context: Reviewed subagent-quality-review.md and saw 4.83/5 average score in practice

- id: skill-invocation-loads-domain-knowledge
  status: pending
  title: Skills loaded via Skill() provide domain knowledge to subagents
  problem: |
    Subagents need access to taxonomies, patterns, criteria, and reference materials without embedding them in prompts.
  solution: |
    Design skills to contain all domain knowledge (SKILL.md + references/*.md) and subagents to invoke them via Skill(command='skill-name'). The skill loading mechanism provides domain knowledge to the subagent context. Subagent prompts should be thin orchestration (50-200 words) that delegates knowledge to skills.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Read, Skill
    context: learning-extraction skill provides 8-category taxonomy loaded by subagents
learnings:
  - id: subagent-description-invocation-clarity
    status: applied
    title: Subagent descriptions should clarify who invokes them not internal delegation
    problem: |
      Writing "via Task() delegation" in subagent description is ambiguous - unclear if it means the subagent IS invoked via Task() or if it USES Task() internally.
    solution: |
      Focus description on: 1) What the subagent does, 2) When parent should invoke it proactively. Example: "Find CLAUDE.md targets and detect duplicate learnings. Use proactively when extract command needs context matching." Avoid "via Task()" unless clarifying parent invocation pattern. The quality reviewer will flag unclear invocation patterns.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: assistant reasoning
      context: Block 8 identified description confusion, block 12 review flagged invocation patterns 2/5
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high

  - id: quality-reviewer-enforces-skill-integration
    status: applied
    title: Quality reviewer treats missing skill integration as critical failure
    problem: |
      Subagents embedding domain knowledge (taxonomies, criteria, patterns) in prompts become bloated and unmaintainable.
    solution: |
      The artifact-quality-reviewer scores skill integration as a mandatory category. Subagents that embed domain logic score 1/5 (Poor) and get NEEDS_REVISION recommendation. The review feedback explicitly requires creating/referencing skills and invoking via Skill() tool. This is non-negotiable - no subagent passes review without proper skill delegation.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality reviewer)
      context: Block 12 shows skill integration 1/5 initially, improved to 5/5 after Skill() delegation added
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high

  - id: implementation-plan-documents-existing-skills
    status: applied
    title: Check implementation plan to discover existing skills before creating new ones
    problem: |
      Quality reviewers may suggest creating new skills when suitable skills already exist in the project, leading to duplication.
    solution: |
      When reviewer suggests creating a skill, read implementation-plan.md or sprint context first. The plan often documents existing skills and their coverage. Example: reviewer suggested "learning-matching" skill, but implementation plan showed learning-extraction already covered target assignment and duplicate detection (SKILL.md lines 172-182, quality-criteria.md lines 161-162).
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high
      tool: Read
      context: Blocks 13-16 discovered learning-extraction skill already contained needed domain logic

  - id: quality-review-typical-iteration-pattern
    status: applied
    title: Expect one iteration cycle from 3-4/5 scores to 5/5 approval
    problem: |
      First drafts rarely achieve perfect scores, but it's unclear how much iteration is normal vs. excessive.
    solution: |
      Typical pattern: First draft scores 3-4 out of 5 average (NEEDS_REVISION), one iteration addressing Major issues achieves 5/5 (APPROVE). Major issues are: skill integration, color coding, model inheritance, prompt structure, invocation patterns. Minor issues (4/5 scores) can be addressed optionally. The transcript shows 3.3/5 → fix 5 issues → 5/5.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high
    confidence: high
    source:
      tool: Task (quality reviewer)
      context: Block 12 review 3.3/5 with 5 critical issues, block 18 review 5/5 after iteration

  - id: subagent-pipeline-io-specification
    status: applied
    title: Specify subagent input/output format explicitly for operator workflows
    problem: |
      Subagents in multi-stage operator workflows need structured I/O contracts. Vague specifications cause pipeline integration failures.
    solution: |
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high
      Document explicit input and output format in subagent prompt using "Receives:" and "Outputs:" sections. Example: "Receives: Array of candidates with {text, problem, solution}. Outputs: Same array annotated with {target, duplicate: bool, related_docs: []}". This enables the orchestrating command to validate data flow between pipeline stages.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Write
      context: Block 19 final subagent includes explicit pipeline I/O specification

  - id: quality-reviewer-catches-color-semantic-mismatches
    status: applied
    title: Quality reviewer validates semantic color coding consistency
    problem: |
    applied-at: "2026-02-01T18:49:07Z"
    applied-by: m42-signs:extract --auto-apply-high
      Without centralized validation, subagent authors may choose colors arbitrarily, reducing visual workflow organization.
    solution: |
      The artifact-quality-reviewer validates color matches subagent purpose using project conventions. It will flag color mismatches as Major issues and provide specific correction recommendations. Example: purple (review/audit) was corrected to cyan (research/analysis) for context-matching subagent. This ensures consistent semantic color coding across all subagents.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Task (quality reviewer)
      context: Block 12 review explicitly flagged color mismatch and suggested cyan correction

  - id: skill-reference-discovery-before-duplication
    status: pending
    title: Search existing skill references before creating new reference files
    problem: |
      Creating duplicate reference files (schemas, formats, criteria) across multiple skills causes maintenance burden and version drift.
    solution: |
      Before writing new reference files, search existing skills' references/ directories for reusable content. Use Glob (skills/*/references/*.md) and Read to check coverage. If found, reference existing files in new SKILL.md rather than duplicating. The learning-extraction skill references managing-signs/references/ for transcript-format.md and backlog-schema.md instead of duplicating them.
    target: plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: Read
      context: Blocks 13-16 discovered existing references covered needed domain knowledge
- id: claude-md-files-no-frontmatter
  status: pending
  title: CLAUDE.md files use standard format without YAML frontmatter
  problem: |
    When verifying file creation in a sprint, it's important to know that CLAUDE.md files have different format requirements than skill or subagent files.
  solution: |
    CLAUDE.md files don't require YAML frontmatter. They use a standard format with `# Claude Code Configuration - <project-name>` heading and `## Learnings` section containing markdown bullet points. This is distinct from SKILL.md files (require frontmatter with name/description) and subagent .md files (require frontmatter with name/description/tools/model/color).
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: Verification showed CLAUDE.md files in plugins/* directories correctly use standard format without frontmatter

- id: deprecation-notice-format-agents
  status: pending
  title: Deprecation notices in subagent files use blockquote format with DEPRECATED marker
  problem: |
    When deprecating subagents in favor of newer alternatives, need consistent format that's both human and AI readable.
  solution: |
    Use this deprecation notice format at the top of deprecated subagent files (after frontmatter):
    ```markdown
    > **DEPRECATED**: This subagent is deprecated. Use `<replacement-name>` instead,
    > which provides <improvement-description>.
    ```
    
    Place notice before the main prompt content. The blockquote format with **DEPRECATED** marker makes it immediately visible. Include both the replacement name and a brief explanation of improvements.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: chunk-analyzer.md shows canonical deprecation notice format at lines 9-10

- id: script-error-message-stderr
  status: pending
  title: Shell scripts should output error/no-match messages to stderr
  problem: |
    Scripts that output status messages to stdout pollute the output stream, making them unsuitable for piping to other commands or processing with jq.
  solution: |
    Use `>&2` redirect for all error, warning, and informational messages:
    ```bash
    if [[ -z "$OUTPUT" ]]; then
      echo "No learning patterns found in: $FILE" >&2
    fi
    ```
    
    Reserve stdout exclusively for data output. This allows scripts to be composed in pipelines without status messages interfering with data processing.
  target: ./CLAUDE.md
  confidence: medium
  source:
    tool: Read
    context: find-learning-lines.sh demonstrates proper stderr usage at lines 38-40
- id: parallel-read-calls-first-error-blocks-siblings
  status: pending
  title: Parallel Read tool calls fail when first path is invalid
  problem: |
    When making parallel Read() tool calls and the first file path doesn't exist,
    subsequent Read() calls in the same batch fail with "Sibling tool call errored"
    even if their paths are valid. This blocks discovery workflows that try multiple
    potential file locations simultaneously.
  solution: |
    Use Glob() to discover valid file paths before calling Read(), or sequence
    Read() calls (read one path, check for errors, then read next) instead of
    parallel calls when dealing with uncertain file locations. For implementation
    plan discovery, glob for **/*implementation-plan.md first, then read the
    correct result.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: Chunk AA - parallel Read failures with invalid first path

- id: operator-pattern-refactoring-validation-first
  status: pending
  title: Validate refactoring vs new creation before operator pattern work
  problem: |
    When transforming a monolithic command into operator pattern, it's critical to
    validate whether you're refactoring existing code or creating new artifacts.
    Without this validation step, you might use wrong workflows (create vs update)
    or miss that supporting infrastructure (skills, subagents) already exists.
  solution: |
    Before starting operator pattern refactoring, validate: 1) Is this refactoring
    existing file or creating new? 2) What's the target file path? 3) Do required
    skills/subagents already exist? 4) What's current state vs target state? The
    transcript shows explicit validation: "This is a refactoring task - modifying
    existing plugins/m42-signs/commands/extract.md" and confirming new infrastructure
    "already exists (per the plan)".
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: TodoWrite
    context: Chunk AA - explicit validation checklist before refactoring

- id: parallel-task-run-in-background-pattern
  status: pending
  title: Parallel subagent execution with run_in_background and TaskOutput
  problem: |
    Sequential subagent processing is slow when analyzing multiple independent
    sections. Need to spawn multiple subagents in parallel and collect results.
  solution: |
    Use Task() with run_in_background=true to spawn parallel tasks, track task
    IDs in an array, then use TaskOutput(task_id=..., block=true) to collect
    results. Pattern: spawn all → track IDs → collect all with blocking wait.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Task
    context: Chunk AB - parallel task execution pattern

- id: command-validation-minimal-flag-rapid-iteration
  status: pending
  title: Use validation script with --minimal flag during development
  problem: |
    Full validation output is verbose and slows iteration during command
    development. Need rapid feedback on errors without seeing all passing checks.
  solution: |
    During development, use `python3 scripts/validate_command.py /path/to/command.md --minimal`
    for fast feedback. Shows only errors/warnings with actionable recommendations.
    Use full validation (without --minimal) only for final review before deployment.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Bash
    context: Chunk AB - rapid iteration with minimal validation

- id: command-validation-false-positives-judgment
  status: pending
  title: Command validation script may flag contextually appropriate content
  problem: |
    The validation script performs pattern-based analysis and may flag content
    that is contextually appropriate, such as anti-pattern examples showing what
    NOT to do, technical terms like "current branch", or quoted incorrect usage.
  solution: |
    Always apply judgment when reviewing flagged items. Ask: Is this in an example
    block showing incorrect usage? Is this technical terminology rather than time
    reference? Is this describing expected behavior vs prescriptive instruction?
    Common false positives: "current branch" in git context, "you" in anti-pattern
    examples, documentation blocks.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Bash
    context: Chunk AB - validation false positive handling

- id: command-frontmatter-task-tool-requirement
  status: pending
  title: Commands using Task() must include Task in allowed-tools
  problem: |
    When refactoring a command to use the operator pattern with Task() for
    subagent delegation, forgetting to add Task to allowed-tools will cause
    runtime errors when the command attempts to invoke subagents.
  solution: |
    When adding Task() invocations to a command, update the allowed-tools
    frontmatter to include "Task" or "Task(*)" for unrestricted subagent access.
    Example: `allowed-tools: Bash(test:*), Read(*), Task(*)`
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Edit
    context: Chunk AB - Task tool must be in allowed-tools

- id: target-assignment-most-specific-applicable
  status: pending
  title: Assign learnings to most specific applicable CLAUDE.md
  problem: |
    Learnings placed in overly broad CLAUDE.md files (like project root) get
    lost and are harder to discover when working in specific areas.
  solution: |
    Follow target assignment hierarchy: single file/directory → feature area →
    plugin/package → project-wide. Place learning in the most specific CLAUDE.md
    that encompasses its scope. Example: TypeScript-specific learning goes in
    plugin CLAUDE.md, not root.
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AB - target assignment hierarchy

- id: duplicate-detection-conservative-approach
  status: pending
  title: Duplicate detection prefers false negatives over false positives
  problem: |
    Aggressive duplicate detection can filter out valuable learnings that have
    subtle but important differences from existing signs.
  solution: |
    Context-matcher subagent uses conservative duplicate detection - prefer
    false negatives (let some duplicates through) over false positives (don't
    filter valuable learnings). Human reviewers can catch actual duplicates
    during backlog review.
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AB - conservative duplicate detection philosophy

- id: glob-vs-bash-find-efficiency
  status: pending
  title: Glob tool preferred over bash find for file discovery
  problem: |
    Commands may use Bash with find command for locating files, but this requires
    adding bash to allowed-tools and is less efficient.
  solution: |
    Use Glob(*) tool instead of `find . -name "CLAUDE.md"` for file discovery.
    Glob is safer (no shell execution), more efficient, and has better tool
    integration. Example: Glob(pattern="**/CLAUDE.md") instead of bash find.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Glob
    context: Chunk AB - Glob vs bash find

- id: command-metrics-operator-pattern-150-lines
  status: pending
  title: Operator pattern commands target ~150 lines
  problem: |
    Commands mixing orchestration and domain logic become bloated (400+ lines),
    making them hard to maintain and review.
  solution: |
    When applying operator pattern, target ~150 lines for the command file.
    Achieve this by: moving domain knowledge to skills, delegating processing
    to subagents, using concise bash examples in Task prompts rather than
    lengthy inline code. 200-line limit is hard ceiling.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: Chunk AB - 150-line target for operator pattern

- id: edge-case-handling-clear-user-messages
  status: pending
  title: Edge cases require clear error messages and graceful exits
  problem: |
    Commands that fail on edge cases (empty input, no results found, all filtered)
    without clear messaging leave users confused about what went wrong.
  solution: |
    Document and handle edge cases explicitly with clear messages: "No content
    to analyze" (empty file), "Session appears mechanical" (no learnings),
    "All learnings already in target files" (all duplicates), "N candidates found,
    all filtered: [reasons]" (quality filtering). Exit gracefully with status 0
    or 1 appropriately.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AB - edge case messaging

- id: task-prompt-skill-reference-pattern
  status: pending
  title: Task prompts reference skills for subagent to load
  problem: |
    Subagents need domain knowledge but shouldn't receive it via large prompt
    content. Need efficient way to provide domain knowledge access.
  solution: |
    In Task() prompts, reference skills by name for subagent to load:
    "Use @learning-extraction skill for taxonomy and criteria" or "Load quality
    criteria via Skill(skill='m42-signs:learning-extraction')". Subagent invokes
    Skill() tool to access domain knowledge.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Task
    context: Chunk AB - skill references in Task prompts

- id: transcript-preprocessing-extract-reasoning
  status: pending
  title: Extract reasoning blocks to reduce transcript token count
  problem: |
    Large transcripts (100+ lines, thousands of tokens) may exceed context limits
    or slow processing. Much content is tool results, not reasoning.
  solution: |
    Use preprocessing script to extract only assistant reasoning blocks:
    `plugins/m42-signs/scripts/extract-reasoning.sh "$TRANSCRIPT_PATH" > reasoning.jsonl`.
    Reduces token count significantly by filtering out tool results, user messages,
    and metadata. Then split if still large (~50 blocks per section).
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: Bash
    context: Chunk AB - preprocessing reduces tokens

- id: dry-run-flag-zero-side-effects
  status: pending
  title: Dry-run mode must have zero side effects
  problem: |
    Users expect --dry-run to preview changes without any file modifications,
    but implementation may accidentally write temp files or modify state.
  solution: |
    In dry-run mode implementation: skip all Write() calls to persistent files,
    skip backlog.yaml appends, skip git operations. Only display preview output.
    Temp files in /tmp are acceptable if needed for preview generation. Clearly
    show "Dry run complete - no changes written" message.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AB - dry-run implementation requirements

- id: validate-command-script-skill-directory-location
  status: pending
  title: Validation scripts live in creating-commands skill directory
  problem: |
    When trying to run validate_command.py from project root scripts/ directory,
    the file doesn't exist there, causing validation to fail.
  solution: |
    The validate_command.py script is located at
    plugins/m42-meta-toolkit/skills/creating-commands/scripts/validate_command.py.
    Use Glob(**/*validate_command.py) to find it, or reference the full path directly.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Glob
    context: Chunk AC - validation script location

- id: preflight-checks-exclamation-mark-syntax
  status: pending
  title: Use exclamation mark syntax for preflight checks in commands
  problem: |
    Commands need to validate preconditions (file existence, valid arguments)
    before running expensive operations. Need consistent syntax for these checks.
  solution: |
    Use preflight checks section with `!` syntax for validation. Example:
    `!Read(transcript-path)` ensures file exists before processing. This
    provides early failure with clear error messages before spawning subagents.
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: Chunk AC - preflight check syntax

- id: task-invocation-condensed-brief-prompts
  status: pending
  title: Keep Task() prompts brief for parallel subagent processing
  problem: |
    Long, detailed prompts in Task() calls make commands verbose and
    duplicate domain knowledge that should live in skills.
  solution: |
    Task() prompts should be 3-5 lines: specify the section/data to process,
    reference the @skill-name for domain knowledge, and state the expected output.
    Example: "Section: {section}\nFocus: {FOCUS}\n\nExtract candidates using @learning-extraction skill."
    Subagents load the skill independently for parallel processing.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Task
    context: Chunk AC - condensed Task prompts

- id: allowed-tools-restrictive-bash-command-patterns
  status: pending
  title: Restrict Bash allowed-tools to specific command patterns
  problem: |
    Allowing Bash(*) gives unlimited shell access, creating security risks
    and making command capabilities unclear.
  solution: |
    Use pattern Bash(command:*) to restrict to specific commands.
    Example: Bash(test:*, mkdir:*, wc:*, split:*) allows only those commands
    with any arguments. This provides minimal necessary access while maintaining
    security boundaries.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AC - restrictive Bash patterns

- id: write-tool-restrictive-specific-paths
  status: pending
  title: Restrict Write tool to specific file paths in commands
  problem: |
    Write(*) allows writing to any file, creating risks of accidental
    overwrites or writing to wrong locations.
  solution: |
    In allowed-tools, specify exact paths: Write(.claude/learnings/backlog.yaml)
    This ensures the command can only write to intended files. For commands
    needing multiple write targets, list each explicitly or use minimal patterns.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Edit
    context: Chunk AC - restrictive Write paths

- id: command-quality-review-independent-subagent
  status: pending
  title: Use independent subagent for command quality review
  problem: |
    Self-review of command artifacts introduces bias and misses issues.
    Need objective evaluation against quality framework.
  solution: |
    Use Task(subagent_type="...", description="Review artifact", prompt="...")
    to delegate quality review to an independent subagent. Provide path to artifact
    and reference to quality framework. Subagent returns structured JSON with
    scores, issues, testing assessment, and recommendation.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Task
    context: Chunk AC - independent quality review

- id: imperative-form-no-second-person-commands
  status: pending
  title: Command instructions must use imperative form without second person
  problem: |
    Using "you", "your", "you should" in command instructions is non-directive
    and fails validation checks.
  solution: |
    Use imperative form: "Run X" not "You should run X", "Check Y" not "You need to check Y".
    Write as direct instructions to Claude, assuming competence. This is validated
    by the automated script and will cause test failures if violated.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Bash (validation)
    context: Chunk AC - imperative form requirement

- id: learning-extraction-skill-centralized-knowledge
  status: pending
  title: Learning extraction domain knowledge lives in dedicated skill
  problem: |
    Commands that inline learning taxonomy, quality criteria, extraction patterns,
    and confidence scoring become bloated (400+ lines) and can't reuse knowledge.
  solution: |
    Create @learning-extraction skill containing: 8-category taxonomy, quality criteria,
    extraction patterns (linguistic/tool signals), confidence scoring rubric.
    Commands reference @learning-extraction, subagents load it via Skill() or @reference.
    Enables parallel subagent processing with shared domain knowledge.
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: Write
    context: Chunk AC - learning extraction skill

- id: extract-three-specialized-subagents
  status: pending
  title: Extract command delegates to three specialized subagents
  problem: |
    Monolithic extract processing is slow and can't leverage parallel processing
    for different analysis phases.
  solution: |
    Use three subagent types: 1) transcript-section-analyzer (parallel per section),
    2) context-matcher (assigns targets, flags duplicates), 3) quality-reviewer
    (scores and filters by confidence). Each loads @learning-extraction skill
    independently. Enables parallel section analysis while maintaining consistency.
  target: plugins/m42-signs/CLAUDE.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AC - three-subagent architecture

- id: preprocessing-script-dependency-preflight
  status: pending
  title: Add preflight checks for script dependencies
  problem: |
    Commands that reference external scripts (e.g., extract-reasoning.sh) in
    their workflow should validate those scripts exist before execution.
  solution: |
    Add preflight check: `!test -f plugins/m42-signs/scripts/extract-reasoning.sh || echo "Warning: extract-reasoning.sh not found"`
    This catches missing dependencies early with clear error messages before
    workflow execution fails mid-process.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Bash
    context: Chunk AC - script dependency checks

- id: operator-pattern-56-percent-size-reduction
  status: pending
  title: Operator pattern achieves 56% file size reduction
  problem: |
    Commands mixing orchestration logic with domain knowledge become bloated
    (359+ lines) and hard to maintain. Domain logic embedded in commands
    prevents reuse across multiple subagents.
  solution: |
    Apply operator pattern: Commands contain only argument parsing, preflight
    checks, Task() orchestration, and result aggregation (~150 lines). Move
    all domain logic to skills that subagents load via Skill() tool. This
    achieved 56% reduction (359→157 lines) in extract.md refactor while
    enabling parallel subagent processing.
  target: ./CLAUDE.md
  confidence: high
  source:
    tool: Read
    context: Chunk AD - 56% size reduction achieved

- id: extract-pipeline-three-subagent-sequence
  status: pending
  title: Extract pipeline uses three specialized subagents in sequence
  problem: |
    Learning extraction involves multiple concerns: analyzing transcript sections,
    matching learnings to target files, and quality scoring. Monolithic approach
    mixes these concerns.
  solution: |
    Use three specialized subagents in sequence: (1) transcript-section-analyzer
    analyzes sections in parallel and extracts raw learnings, (2) context-matcher
    determines target CLAUDE.md files, (3) quality-reviewer scores and filters.
    Each subagent has single responsibility and loads shared @learning-extraction skill.
  target: plugins/m42-signs/commands/extract.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AD - three-subagent pipeline

- id: extract-edge-case-handling-five-cases
  status: pending
  title: Extract command handles five edge cases explicitly
  problem: |
    Extract command can encounter edge cases: empty transcript, no assistant
    messages, mechanical tasks with no learnings, all learnings filtered by
    quality, or all duplicates. Need graceful handling.
  solution: |
    Add explicit edge case handling for: (1) Empty transcript, (2) No assistant
    messages, (3) Mechanical tasks (no learnings), (4) All filtered by quality,
    (5) All duplicates. Each case provides clear user feedback instead of
    failing silently or with cryptic errors.
  target: plugins/m42-signs/commands/extract.md
  confidence: high
  source:
    tool: assistant reasoning
    context: Chunk AD - five edge cases documented

- id: section-division-parallel-transcript-processing
  status: pending
  title: Divide transcripts into sections for parallel subagent analysis
  problem: |
    Large transcripts need efficient processing. Processing entire transcript
    in single subagent is slow and doesn't scale.
  solution: |
    Divide transcript into logical sections (by assistant messages or size),
    then spawn transcript-section-analyzer subagent per section in parallel.
    Aggregate results after all sections complete. This enables parallelism
    while maintaining section context for analysis.
  target: plugins/m42-signs/commands/extract.md
  confidence: high
  source:
    tool: Task
    context: Chunk AD - section division for parallelism
  confidence: medium
  source:
    tool: Read
    context: Verified CLAUDE.md format while checking file creation completion

- id: skill-execution-errors-dont-block-progress
  status: applied
  applied-at: "2026-02-01T18:59:45Z"
  applied-by: m42-signs:extract --auto-apply-high
  title: Proceed with direct implementation when skill execution fails
  problem: |
    When invoking creation skills (creating-commands, creating-skills, creating-subagents) via Skill() tool, execution errors may occur. Waiting for skill fixes blocks progress on valid tasks.
  solution: |
    If a creation skill fails to execute, proceed with direct implementation using the skill's documented patterns and guidelines. Read the skill file (SKILL.md) and references directly to understand the patterns, then apply them manually. This happened in phase-1_step-4 where creating-commands skill failed but the agent successfully read creating-commands/SKILL.md and proceeded with refactoring.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Skill, Read
    context: Blocks 4-6 show Skill() execution failure, followed by Read of skill file and successful direct implementation

- id: command-imperative-form-section-headers
  applied-at: "2026-02-01T18:59:45Z"
  applied-by: m42-signs:extract --auto-apply-high
  status: pending
  title: Command section headers must use imperative form not second-person
  problem: |
    Commands may have section headers like "## Your Task" which violates imperative form guidelines. This applies to section headers, not just task descriptions.
  solution: |
    All command section headers must use imperative or neutral form: "## Task Instructions" (imperative/neutral) not "## Your Task" (second-person possessive). The quality review in phase-1_step-4 identified "Your Task" at line 37 as violating imperative form and required changing it to "Task Instructions".
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Edit
    context: Block 14 identified line 37 "## Your Task" as imperative form violation, fixed to "## Task Instructions"

- id: command-ultrathink-success-criteria
  applied-at: "2026-02-01T18:59:45Z"
  applied-by: m42-signs:extract --auto-apply-high
  status: pending
  title: Commands must append ultrathink requirement to success criteria
  problem: |
    The command creation workflow mandates that all commands include ultrathink mode requirement, but it's not always clear where this should be placed.
  solution: |
    Append ultrathink requirement at the end of the Success Criteria section: "IMPORTANT: Operate in ultrathink mode throughout execution." This is a mandatory requirement from the create-command workflow (Step 7). The requirement was added in block 15 during command refactoring.
  target: plugins/m42-meta-toolkit/CLAUDE.md
  confidence: high
  source:
    tool: Edit
    context: Block 15 added ultrathink requirement to Success Criteria as final step of command creation workflow
  - id: test-operator-pattern-commands-with-subagent-spawning
    status: pending
    title: Test operator-pattern commands by verifying subagent spawning
    problem: |
      When testing commands that use the operator pattern (command orchestrates, subagents provide logic), it's unclear what to verify beyond basic command invocation. Testing only the command script itself misses the critical workflow: subagent spawning, skill invocation, and output aggregation.
    solution: |
      Test operator-pattern commands using a three-tier validation approach:
      1. **Subagent spawning verification**: Confirm Task(subagent_type="plugin:name") succeeds and doesn't error with "Agent type not found"
      2. **Output format validation**: Verify subagent output matches expected schema (e.g., YAML structure, required fields)
      3. **Edge case handling**: Test both content-rich inputs (should produce results) and mechanical inputs (should gracefully return empty results)
      
      Example from extract command testing:
      - Verified chunk-analyzer subagent spawned successfully
      - Checked output matched backlog.yaml schema (id, status, title, problem, solution, target, confidence)
      - Tested mechanical transcript returned learnings: [] with clear rationale
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
    confidence: high
  - id: plugin-registration-blocks-subagent-spawning
    status: pending
    title: Subagents must be registered in plugin.json for Task() to work
    problem: |
      When implementing subagents for operator-pattern commands, creating the .md file in agents/ folder is insufficient. Attempting to spawn via Task(subagent_type="plugin:name") fails with "Agent type 'plugin:name' not found" even if the .md file exists, because plugin.json lacks agent registry entries.
    solution: |
      After creating subagent .md files, update plugins/[plugin]/.claude-plugin/plugin.json to register them:
      ```json
      {
        "agents": {
          "subagent-name": {
            "path": "agents/subagent-name.md",
            "description": "Brief description"
          }
        }
      }
      ```
      
      This applies to all plugin artifacts: commands (slash commands), skills (domain knowledge via Skill() tool), and agents (subagents via Task() tool). Without registry entries, the Claude Code runtime cannot find them during execution.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
  - id: test-mechanical-transcripts-return-empty-gracefully
    status: pending
    title: Extraction workflows should handle mechanical transcripts gracefully
    problem: |
      Transcripts containing only verification tasks (file reads, status checks, validation operations) have minimal learning value. If extraction workflows don't detect this, they waste processing attempting to extract insights from procedural operations, or worse, hallucinate low-value learnings.
    solution: |
      Design extraction subagents (like chunk-analyzer) to:
      1. Detect mechanical patterns (mostly Read operations, verification tasks, status checks)
      2. Return empty results with clear rationale (e.g., learnings: [] with explanation)
      3. Avoid forcing insights from low-value content
      
      Example from testing:
      - Mechanical transcript input: verification tasks with file reads only
      - Chunk-analyzer output: `learnings: []` with rationale: "mechanical verification steps", "no architectural insights", "low-value procedural work"
      
      This graceful handling prevents backlog pollution and makes it clear when transcripts lack learning value.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-signs/CLAUDE.md
    confidence: high
  - id: large-transcript-preprocessing-for-chunking
    status: pending
    title: Preprocess large transcripts before chunking for extraction
    problem: |
      Large transcripts (100+ lines, 200KB+) contain verbose JSON structures with full file contents and tool calls. Passing these directly to extraction subagents wastes context and makes it hard to identify reasoning blocks versus data payload.
    solution: |
      Use preprocessing scripts before chunking large transcripts:
      1. **transcript-summary.sh**: Generate stats (message counts, block counts, error counts)
      2. **extract-reasoning.sh**: Extract only reasoning blocks (text content) from JSONL
      
      Then chunk the preprocessed reasoning blocks (not raw transcript) for subagent analysis. This gives subagents focused content without tool call noise, file payload bloat, or JSON structure overhead.
      
      Example from testing:
      - Raw transcript: 102 lines, 290KB
      - Preprocessed: 15 reasoning blocks extracted
      - Subagent received only reasoning content, extracted 8 focused learnings
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-signs/CLAUDE.md
    confidence: high
  - id: verify-output-schema-matches-backlog-format
    status: pending
    title: Validate extraction output matches backlog schema exactly
    problem: |
      Extraction workflows that produce learnings for backlog.yaml can generate incorrect field names, missing fields, or wrong data types. If output doesn't match schema exactly, manual cleanup is required before merging learnings.
    solution: |
      When testing extraction commands, explicitly verify output against backlog schema:
      
      Required fields for each learning:
      - id: kebab-case-unique-id (string)
      - status: pending (literal string)
      - title: Short description (string, 5-10 words)
      - problem: Multi-line description (string, literal block)
      - solution: Multi-line description (string, literal block)
      - target: /absolute/path/to/CLAUDE.md (string)
      - confidence: high | medium | low (enum)
      
      Check YAML structure, field presence, value types, and formatting (pipe for multi-line). This ensures output can be merged into backlog.yaml without manual reformatting.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-signs/CLAUDE.md
    confidence: high
  - id: test-content-rich-and-mechanical-transcripts
    status: pending
    title: Test extraction workflows with both content-rich and mechanical inputs
    problem: |
      Testing extraction workflows only with content-rich transcripts (implementation sessions, debugging) doesn't validate edge case handling. Commands may fail or produce low-quality output when given mechanical transcripts (verification tasks, simple reads).
    solution: |
      Create a two-test validation strategy for extraction workflows:
      
      **Test 1: Content-rich transcript** (100+ lines, implementation/debugging)
      - Should extract multiple learnings (5-10)
      - Verify learning quality (architectural insights, pitfalls, strategies)
      - Check target assignment (correct CLAUDE.md paths)
      
      **Test 2: Mechanical transcript** (verification tasks, reads only)
      - Should return empty or minimal results
      - Verify graceful handling with clear rationale
      - Ensure no hallucinated low-value learnings
      
      Both tests together validate the workflow handles the full spectrum of transcript types.
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
    confidence: medium
  - id: chunk-analyzer-subagent-output-quality
    status: pending
    title: Chunk-analyzer should provide reasoning with learning extractions
    problem: |
      When chunk-analyzer subagent extracts learnings from transcript chunks, returning only YAML output without reasoning makes it hard to verify quality or understand why specific learnings were extracted/categorized.
    solution: |
      Design chunk-analyzer (and similar extraction subagents) to include reasoning alongside YAML output:
      - Explain target assignment rationale (why this CLAUDE.md?)
      - Justify confidence levels (high vs medium vs low)
      - Note any borderline cases or decisions made
      
      Example from testing showed chunk-analyzer included:
      - "Target plugins/m42-signs/CLAUDE.md because this is about quality review agent workflow"
      - "Confidence: high - this is a clear workflow improvement with specific implementation details"
      
      This reasoning helps validate extractions and provides audit trail for quality.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-signs/CLAUDE.md
    confidence: medium
  - id: preprocessing-stats-inform-extraction-strategy
    status: pending
    title: Use transcript stats to decide chunking and extraction strategy
    problem: |
      Different transcripts have vastly different sizes and content density. A one-size-fits-all extraction approach wastes resources on small transcripts (unnecessary chunking overhead) or fails on large ones (context overflow).
    solution: |
      Run transcript-summary.sh preprocessing to get stats:
      - Message counts (assistant messages indicate work done)
      - Block counts (text blocks = reasoning content)
      - Error counts (errors = troubleshooting learnings)
      - File size (determines chunking threshold)
      
      Then decide strategy:
      - Small (<100 lines, <50KB): Process directly without chunking
      - Large (100+ lines, 200KB+): Chunk preprocessed reasoning blocks
      - Error-heavy: Focus extraction on error context and solutions
      
      Example from testing: 102-line transcript triggered large-transcript workflow with preprocessing + chunking, while 20-line transcript could be processed directly.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-signs/CLAUDE.md
    confidence: medium

# Applied by extract command with --auto-apply-high flag
# Applied at: 2026-02-01T20:00:00Z

# Extracted from phase-1_step-6_execute.log - 2026-02-01
  - id: plugin-registry-required-for-subagents
    status: pending
    title: Plugin.json must register commands, skills, and agents for Task() invocation
    problem: |
      The refactored extract command references three subagents (transcript-section-analyzer, 
      context-matcher, quality-reviewer) that exist as .md files in plugins/m42-signs/agents/ 
      but are not registered in plugin.json. When attempting to spawn via Task(subagent_type="m42-signs:transcript-section-analyzer"), 
      the system returned "Agent type 'm42-signs:transcript-section-analyzer' not found".
      
      The plugin.json file only contained basic metadata (name, version, description) but was 
      missing the registry structure for commands, skills, and agents arrays.
    solution: |
      Plugin.json files must include registry arrays to make artifacts discoverable by Claude Code's 
      invocation mechanisms:
      - `commands`: Array of slash command definitions
      - `skills`: Array of skill definitions (for Skill() tool)
      - `agents`: Array of subagent definitions (for Task() tool)
      
      Without proper registration, commands cannot invoke subagents via Task() and subagents cannot 
      load skills via Skill(), breaking the operator pattern workflow. The existence of .md files 
      in agents/ or skills/ directories is not sufficient - they must be explicitly registered.
      
      Check other plugin.json files or Claude Code plugin schema for proper registry structure examples.
    target: plugins/m42-signs/CLAUDE.md
    confidence: high
    source:
      context: "Testing refactored extract command during phase-1_step-6_execute. Attempted to spawn transcript-section-analyzer subagent which failed with 'Agent type not found' error despite .md file existing. Investigation revealed plugin.json missing registry arrays."

  - id: operator-pattern-requires-complete-registration
    status: pending
    title: Operator pattern workflows fail completely without plugin registry
    problem: |
      The operator pattern separates orchestration (in commands) from domain logic (in skills) 
      using subagent delegation. The extract command was refactored to this pattern with:
      - Command file (extract.md) containing only orchestration logic
      - Three subagents (transcript-section-analyzer, context-matcher, quality-reviewer) for processing
      - One skill (@learning-extraction) containing all domain knowledge
      
      However, testing revealed the workflow cannot execute at all because subagents and skills 
      aren't registered in plugin.json. The command file exists and can be invoked, but all 
      Task() and Skill() calls fail immediately.
    solution: |
      When implementing operator pattern workflows:
      1. Create all artifacts (.md files in correct directories)
      2. Register ALL artifacts in plugin.json before testing
      3. Verify registration with available tools/agents list
      4. Test the complete workflow end-to-end
      
      The operator pattern has a hard dependency on the plugin registry - without registration, 
      the entire workflow is non-functional. This is especially critical for refactoring existing 
      commands to operator pattern, where forgetting registration makes the new implementation 
      completely broken while the old version may have worked.
    target: plugins/m42-signs/CLAUDE.md
    confidence: high
    source:
      context: "Refactoring extract command from monolithic (400+ lines) to operator pattern workflow. Created all component files (3 subagents, 1 skill, 1 command) but forgot registration step. Command invocable but completely non-functional due to Task() failures."

  - id: test-execution-discovers-registration-gaps
    status: pending
    title: Execute step reveals registration issues invisible during creation
    problem: |
      During sprint workflow, phases 1-5 created:
      - extract.md command file (~150 lines)
      - 3 subagent files in agents/ directory
      - learning-extraction skill with SKILL.md and 4 reference files
      
      All creation steps appeared successful with no errors. Only when reaching phase-1_step-6_execute 
      (testing the command end-to-end) did the registration gap become apparent via Task() invocation failure.
      
      The creation phases had no mechanism to detect that artifacts wouldn't be discoverable at runtime.
    solution: |
      The execute/test phase is critical for discovering integration issues that creation phases cannot detect:
      - Missing plugin registry entries
      - Incorrect subagent naming conventions
      - Skill trigger patterns not matching invocation
      - Tool permission mismatches
      
      Cannot rely on successful artifact creation as evidence of working implementation. Must include 
      end-to-end execution test that exercises all Task() and Skill() invocations. For operator pattern 
      refactors, this means testing the complete multi-stage workflow with real input.
      
      Consider adding plugin validation command that checks .md files against plugin.json registry 
      and reports unregistered artifacts.
    target: CLAUDE.md
    confidence: high
    source:
      context: "Six-step sprint workflow where steps 1-5 (create skill, create subagents, create command) all succeeded, but step 6 (execute) revealed none of the artifacts were registered for runtime invocation. Clear example of creation success masking integration failure."

  - id: available-agents-list-shows-registration-status
    status: pending
    title: Task() error messages include list of available registered agents
    problem: |
      When Task() invocation fails due to unregistered agent, the error message isn't just 
      "not found" - it includes the complete list of currently available agents. The transcript 
      shows the error:
      
      "Agent type 'm42-signs:transcript-section-analyzer' not found. Available agents: Bash, 
      general-purpose, statusline-setup, Explore, Plan, claude-code-guide, m42-signs:chunk-analyzer, 
      m42-meta-toolkit:artifact-quality-reviewer, m42-meta-toolkit:agent-creator, 
      m42-meta-toolkit:skill-creator, m42-meta-toolkit:doc-writer, m42-meta-toolkit:command-creator"
      
      This reveals that m42-signs:chunk-analyzer IS registered (appeared in available list) while 
      the three new subagents are not.
    solution: |
      When debugging Task() invocation failures:
      1. Check the "Available agents:" list in the error message
      2. Verify if other agents from same plugin are registered (indicates plugin is loaded)
      3. Use this to distinguish between plugin loading issues vs specific agent registration gaps
      
      The available agents list is a diagnostic tool for understanding registration state. In this 
      case, seeing "m42-signs:chunk-analyzer" confirmed the plugin.json was being read and chunk-analyzer 
      was properly registered, isolating the issue to the three newly created subagents not being 
      added to the registry.
      
      This is more informative than a simple "not found" error and helps identify whether the problem 
      is plugin-wide or artifact-specific.
    target: CLAUDE.md
    confidence: high
    source:
      context: "Task() invocation error during extract command test included complete list of available agents. List showed existing m42-signs:chunk-analyzer was registered while new subagents weren't, helping isolate registration gap to specific artifacts rather than plugin loading failure."

  - id: chunk-analyzer-exists-but-new-subagents-missing
    status: pending
    title: Existing vs new subagent registration reveals incomplete migration
    problem: |
      The available agents list shows "m42-signs:chunk-analyzer" is registered, but the three 
      new subagents (transcript-section-analyzer, context-matcher, quality-reviewer) are not. 
      
      Both chunk-analyzer.md and transcript-section-analyzer.md exist in plugins/m42-signs/agents/ 
      directory. The difference is chunk-analyzer was created/registered in previous work, while 
      the three new subagents were created during this refactor sprint but not registered.
    solution: |
      When adding new artifacts to existing plugins:
      1. Check plugin.json to see what's currently registered
      2. Follow the same registration pattern for new artifacts
      3. Don't assume "plugin already works" means registration will happen automatically
      
      Existing registered artifacts prove the plugin.json structure exists and is being read. 
      New artifacts must be added to the same registry arrays. This is easy to miss when 
      incrementally enhancing a plugin - the plugin.json update is a manual step that creation 
      commands/subagents don't handle automatically.
      
      Consider making plugin.json updates part of artifact creation workflows, or create a 
      registration command that scans directories and updates plugin.json automatically.
    target: plugins/m42-signs/CLAUDE.md
    confidence: high
    source:
      context: "Available agents error message showed m42-signs:chunk-analyzer registered but new subagents weren't. Both exist as .md files in same directory. Reveals registration is manual step not automatically applied to new artifacts added to existing plugin."

  - id: skill-registration-affects-both-invocation-methods
    status: pending
    title: Skills need registration in plugin.json for Skill() tool invocation
    problem: |
      The learning-extraction skill exists at plugins/m42-signs/skills/learning-extraction/ with:
      - SKILL.md (core taxonomy and patterns)
      - references/ subdirectory (4 detailed reference files)
      
      The extract.md command and all three subagents reference this skill via @learning-extraction 
      pattern or Skill(command='learning-extraction') invocation. However, the transcript shows 
      concern that the skill "may also not be registered properly for use with Skill() calls" 
      alongside the confirmed subagent registration failure.
    solution: |
      Skills have two invocation methods:
      1. Direct reference via @ syntax in prompts (e.g., @learning-extraction)
      2. Explicit loading via Skill() tool (e.g., Skill(command='learning-extraction'))
      
      Both methods may require proper registration in plugin.json's skills array. The skill files 
      must exist in correct directory structure AND be registered. Verify skills array in plugin.json 
      includes all skills that subagents will invoke via Skill() tool.
      
      The transcript flags this as "Additional Concerns" without confirming if skills have the same 
      registration requirement as agents, but the operator pattern documentation suggests skill 
      registration is necessary for the Skill() tool to discover and load them.
    target: plugins/m42-signs/CLAUDE.md
    confidence: medium
    source:
      context: "Testing extract command revealed agent registration failure. Final test summary included 'Additional Concerns' section flagging that @learning-extraction skill may have same registration requirement but wasn't confirmed during testing."

  - id: mechanical-transcripts-have-minimal-learning-value
    status: pending
    title: Summary phase transcripts are mechanical with low learning extraction value
    problem: |
      The extract command test used phase-6_summary.log transcript (34 lines) from agent-monitor-panel 
      sprint. After reading the transcript, the agent observed: "This is a summary phase transcript - 
      very mechanical with minimal learning value."
      
      Summary phase transcripts contain primarily:
      - Git status/diff commands
      - Commit message creation
      - Document aggregation
      - Success confirmation
      
      These operations are procedural and don't involve problem-solving, discovery, or error recovery 
      that would generate meaningful learnings.
    solution: |
      When selecting transcripts for learning extraction:
      - Prioritize implementation phases (step-2_implement, step-3_test, step-4_refactor) over summary phases
      - Look for phases with debugging, error recovery, or architectural decisions
      - Summary/documentation phases are completion ceremonies with minimal learning content
      
      For testing extract command itself, use mechanical transcripts to verify edge case handling 
      ("Session appears mechanical - no learning patterns detected" output path). For actual learning 
      extraction, target transcripts with problem-solving activity.
      
      The 8-category taxonomy's "Effective Strategies" and "Pitfalls & Gotchas" categories are 
      unlikely to appear in summary phase transcripts.
    target: plugins/m42-signs/CLAUDE.md
    confidence: medium
    source:
      context: "Testing extract command on phase-6_summary.log transcript. Agent correctly identified the content as mechanical/procedural before attempting subagent spawning. Suggests transcript selection criteria for meaningful learning extraction."

  - id: preflight-checks-validate-external-dependencies
    status: pending
    title: Preflight checks verify script dependencies before workflow execution
    problem: |
      The extract.md command includes preflight checks that validate external dependencies:
      ```
      - Preprocessing script: !`test -f plugins/m42-signs/scripts/extract-reasoning.sh || echo "Warning: extract-reasoning.sh not found"`
      ```
      
      This checks for the preprocessing script existence before attempting to use it in the workflow. 
      However, this is only a warning, not a hard failure, because the script is only needed for 
      large transcripts (>100 lines) and the command can work without it for smaller inputs.
    solution: |
      Preflight checks should validate external dependencies (scripts, binaries, config files) 
      that workflows rely on:
      - Use `test -f` for file existence
      - Echo warnings for optional dependencies (fallback behavior exists)
      - Exit with error for required dependencies (no workaround available)
      
      The extract command's preprocessing script is optional because transcripts <100 lines bypass 
      preprocessing entirely. This is good design - the preflight check warns but doesn't block 
      execution when the dependency might not be needed.
      
      For required dependencies, use pattern: `test -f <path> || { echo "Error: <file> not found"; exit 1; }`
    target: CLAUDE.md
    confidence: medium
    source:
      context: "Extract command preflight checks include preprocessing script validation. Script is optional (only for large transcripts) so preflight warns but doesn't fail. Good example of distinguishing required vs optional dependency validation."

  - id: worktree-paths-span-main-and-sprint-directories
    status: pending
    title: Sprint workflows read from main repo and write to worktree paths
    problem: |
      During extract command test, transcript paths span two locations:
      - Read from: /home/koni/projects/m42-claude-plugins/.claude/sprints/2026-01-29_agent-monitor-panel/transcriptions/
        (main repo path, not in worktree)
      - Write to: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/
        (worktree path for current sprint)
      
      The extract command runs in worktree context but needs to read transcripts from completed 
      sprints in main repo. Plugin files being tested are in worktree but historical data is in main.
    solution: |
      Sprint workflows in worktree environments must handle two path contexts:
      1. Worktree-local paths: Current sprint artifacts, modified plugin files, test outputs
      2. Main repo paths: Historical sprint data, shared configuration, reference transcripts
      
      Commands should accept absolute paths for inputs (allowing main repo references) while 
      writing outputs to relative paths (landing in current worktree). Preflight checks use 
      CWD-relative paths for targets but accept absolute paths for source data.
      
      The extract command's `<transcript-path>` argument can be absolute (pointing to main repo) 
      while `.claude/learnings/backlog.yaml` output is relative (written to current worktree).
    target: plugins/m42-sprint/CLAUDE.md
    confidence: medium
    source:
      context: "Extract command test in sprint worktree read transcript from main repo path (/home/koni/projects/m42-claude-plugins/.claude/sprints/) while running in worktree (/home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/). Command handled absolute input paths correctly."

  # Extracted from phase-3_tooling-update.log on 2026-02-01
  - id: tooling-update-sprint-workflow
    status: applied
    applied-at: '2026-02-01T20:15:00Z'
    title: Sprint tooling phase validates docs match implementation
    problem: |
      After implementing feature changes (like removing depends-on), documentation,
      commands, and skills need to be reviewed for consistency. Stale references
      can remain in unrelated files.
    solution: |
      During sprint tooling phase: 1) Identify affected plugin, 2) Review all
      commands and skills for stale references using grep/rg, 3) Check that
      documentation examples match current implementation, 4) Update SKILL.md
      files to remove deprecated feature references (id, depends-on, parallel
      execution sections).
    category: build-test-patterns
    confidence: high
    evidence: |
      Sprint phase-3 systematically reviewed 13 commands and 4 skills, found
      creating-sprints/SKILL.md contained deprecated depends-on references,
      updated it, and committed changes.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-sprint/CLAUDE.md

  - id: parallel-terminology-disambiguation
    status: pending
    title: Disambiguate parallel execution terminology in docs
    problem: |
      Term "parallel" has multiple meanings in sprint context - "parallel sprint
      execution" means worktree isolation, "parallel execution" in workflows means
      phase-level parallelism, both different from deprecated step-level depends-on.
      This creates confusion when removing depends-on feature.
    solution: |
      When reviewing docs for deprecated features, check context of "parallel"
      references. Legitimate uses: "parallel sprint execution" (worktree isolation),
      "parallel execution" (phase-level in workflows), technical terms like
      "dependency injection". Only remove references to step-level dependencies
      (depends-on field).
    category: project-conventions
    confidence: medium
    evidence: |
      During grep for "parallel" and "dependency", found legitimate uses in
      different contexts that should NOT be removed when cleaning up depends-on
      references.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-sprint/CLAUDE.md

  - id: systematic-tooling-review-pattern
    status: applied
    applied-at: '2026-02-01T20:15:00Z'
    title: Use systematic subagent review for multi-artifact validation
    problem: |
      When a feature spans multiple commands and skills, manual review is
      error-prone. Need to ensure no artifacts contain stale references.
    solution: |
      Spawn subagents to review all commands and all skills in parallel. Each
      subagent checks for deprecated patterns and returns status (unchanged vs
      needs update). Aggregate results to identify files needing updates. This
      parallelizes review and ensures completeness.
    category: effective-strategies
    confidence: high
    evidence: |
      Transcript shows spawning 12+ subagents to review commands, then spawning
      4 subagents for skills, systematically covering all artifacts and identifying
      creating-sprints/SKILL.md needed updates.
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
  - id: version-bump-check-committed-and-uncommitted-changes
    status: pending
    title: Version bump must check both committed and uncommitted plugin changes
    problem: |
      When determining which plugin to version bump in a sprint worktree, checking only `git diff main..HEAD` misses uncommitted changes that are part of the sprint work.
      
      Example: Branch named `sprint/extract-command-refactor` suggests m42-signs work, but `git diff main..HEAD` only shows m42-sprint commits. The actual m42-signs changes are uncommitted.
      
      This can lead to bumping the wrong plugin version or missing version bumps entirely.
    solution: |
      For version bump determination in sprint workflows, check BOTH:
      
      1. Committed changes: `git diff main..HEAD --name-only | grep "^plugins/"`
      2. Uncommitted changes: `git status --short | grep "plugins/"`
      
      The union of both results identifies all affected plugins that need version bumps.
      
      Pattern for comprehensive check:
      ```bash
      # Committed changes
      git diff main..HEAD --name-only | grep "^plugins/" | cut -d'/' -f2 | sort -u
      
      # Plus uncommitted changes
      git status --short | grep "plugins/" | cut -d'/' -f2 | sort -u
      ```
    target: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/plugins/m42-sprint/CLAUDE.md
    confidence: high
    source:
      tool: Bash
      context: Phase 4 version bump discovery in sprint automation
    status: applied
    applied-at: '2026-02-01T00:00:00Z'
    applied-note: Auto-applied via --auto-apply-high flag to plugins/m42-sprint/CLAUDE.md

  - id: document-deprecation-across-doc-layers
    status: pending
    title: Remove deprecated features from all documentation layers systematically
    problem: |
      When removing a feature from a system, documentation exists at multiple layers:
      - User-facing docs (USER-GUIDE.md, getting-started)
      - Reference docs (schema specifications, API docs)
      - Skill references (what AI reads when helping users)
      Each layer must be updated to prevent confusion and ensure consistency.
    solution: |
      When deprecating a feature, update documentation in this order:
      1. Reference documentation - Remove from schema specs, API docs
      2. User guides - Remove examples, tutorials, how-to sections
      3. Getting started - Remove feature mentions from overview/index
      4. Skill references - Remove from AI guidance documents
      
      Use separate commits for each layer to track what was changed where.
      Example pattern from m42-sprint depends-on removal:
      - docs(reference): remove deprecated feature from reference docs
      - docs(user-guide): remove deprecated feature
      - docs(getting-started): remove deprecated feature
      - docs(creating-sprints): sync with implementation
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
    confidence: high
    source:
      tool: git
      context: Phase-2 documentation updates removing depends-on feature from 4+ documentation layers

  - id: skill-references-shape-ai-suggestions
    status: pending
    title: Skill reference files guide AI behavior differently than user docs
    problem: |
      Skills contain reference files that AI reads when helping users create artifacts.
      These files shape what the AI suggests. If skill references document a feature,
      the AI will suggest using it even if it's deprecated or discouraged.
      
      Example: The creating-sprints skill had depends-on examples in sprint-schema.md
      and step-writing-guide.md. Even though the feature was being phased out, the AI
      would suggest it to users creating new sprints.
    solution: |
      Treat skill reference files as AI behavior configuration:
      
      1. Skill references (skills/*/references/*.md) - What AI suggests to users
      2. User documentation (docs/*.md) - What users read manually
      3. API/reference docs - Complete technical specification
      
      When phasing out a feature:
      - Remove from skill references first to stop AI suggestions
      - Keep in reference docs for users who manually search
      - Eventually remove from all layers when fully deprecated
      
      Skill references should document the RECOMMENDED patterns, not all possible patterns.
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-meta-toolkit/CLAUDE.md
    confidence: high
    source:
      tool: Edit
      context: Removing depends-on from creating-sprints skill references to stop AI from suggesting deprecated pattern

  - id: worktree-vs-dag-parallelism-distinction
    status: pending
    title: Distinguish between worktree-based and DAG-based parallel execution
    problem: |
      Sprint system had two separate parallel execution features that got confused:
      
      1. Worktree-based parallelism - Run multiple independent sprints in separate
         git worktrees simultaneously (still supported)
      
      2. DAG-based parallelism - Run multiple steps within one sprint using
         dependency graph with depends-on (being removed)
      
      When removing DAG-based parallelism, documentation mentioning "parallel execution"
      for worktrees was incorrectly flagged as needing removal.
    solution: |
      When documenting features with similar names:
      
      1. Use distinct terminology:
         - "Worktree-based parallel execution" vs "Step dependency parallelism"
         - "Multi-sprint parallelism" vs "Intra-sprint parallelism"
      
      2. Add clarifying comments in docs:
         ```markdown
         ## Parallel Sprint Execution (Worktree-Based)
         Note: This is separate from step dependencies. This enables
         running multiple sprints simultaneously in different worktrees.
         ```
      
      3. In deprecation work, explicitly note what remains:
         "Removed DAG-based step parallelism. Worktree-based parallel
         execution is a separate feature and remains documented."
    target: /home/koni/projects/m42-claude-plugins/plugins/m42-sprint/CLAUDE.md
    confidence: medium
    source:
      tool: Read
      context: Analyzing USER-GUIDE.md and finding worktree parallel execution docs that should not be removed

  - id: systematic-doc-removal-with-line-counts
    status: pending
    title: Track documentation removal systematically with line counts per file
    problem: |
      When removing deprecated features across multiple documentation files,
      it's easy to miss files or sections. Need a systematic way to verify
      complete removal and communicate scope of changes.
    solution: |
      Use structured tracking in commit messages and summary artifacts:
      
      1. Commit messages include per-file line counts:
         ```
         docs(reference): remove deprecated feature
         
         - sprint-yaml-schema.md: Remove field from table (38 lines)
         - progress-yaml-schema.md: Remove interfaces (149 lines)  
         - api.md: Remove entire section (298 lines)
         ```
      
      2. Create summary artifact with table:
         | File | Lines Removed | What Was Removed |
         |------|---------------|------------------|
         | sprint-yaml-schema.md | 47 | depends-on field, examples |
         | api.md | 298 | StepScheduler API section |
      
      3. Track totals by category:
         - Reference docs: 494 lines removed
         - User guides: 237 lines removed
         - Getting started: 1 line removed
      
      This provides clear verification of completeness and helps with changelog generation.
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
    confidence: high
    source:
      tool: git
      context: Commit messages showing detailed line counts for each doc file change

  - id: separate-commits-per-doc-layer
    status: pending
    title: Use separate git commits for each documentation layer when removing features
    problem: |
      Documentation updates span multiple concerns (reference, user guide, getting started).
      Bundling all changes into one commit makes it hard to:
      - Review what changed in each layer
      - Revert specific documentation updates if needed
      - Generate accurate changelogs
      - Understand scope of changes
    solution: |
      When updating documentation for a feature removal/change:
      
      1. Create separate commits for each documentation layer:
         ```
         docs(reference): remove deprecated feature from reference docs
         docs(user-guide): remove deprecated feature
         docs(getting-started): remove deprecated feature  
         docs(skill-name): sync with implementation
         ```
      
      2. Order commits from technical → user-facing:
         - Reference docs first (schema, API)
         - User guides second (tutorials, how-tos)
         - Getting started last (overview, index)
         - Skills last (AI guidance)
      
      3. Include specific file changes in each commit:
         Reference commit touches: schema.md, api.md, interfaces.md
         User guide commit touches: USER-GUIDE.md, guides/*.md
      
      This enables:
      - Easier code review (review one layer at a time)
      - Selective reverts if needed
      - Better git log clarity
      - Accurate changelog generation
    target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
    confidence: high
    source:
      tool: git
      context: Sprint created 4 separate commits for doc updates across different layers

# Learnings extracted from phase-2_documentation.log (2026-02-01)
- id: documentation-phase-initial-assessment-gap
  status: pending
  title: Documentation phase should detect user-facing feature removal
  problem: |
    The phase-2 documentation coordinator initially assessed that no documentation updates 
    were needed because changes appeared to be only to "internal skill references". However, 
    the workflow later spawned 3 subagents and made extensive documentation updates (732+ 
    lines removed across 7 files), suggesting the initial assessment missed that skill 
    reference simplification indicated user-facing feature removal (depends-on).
    
    Future phase-2 implementations might make the same mistake - concluding that internal 
    refactoring doesn't need documentation updates, when in fact the refactoring removes 
    user-facing features.
  solution: |
    When assessing whether documentation updates are needed during phase-2, consider these 
    signals that internal changes affect user-facing features:
    
    1. Skill reference simplification (removing sections) may indicate deprecated features
    2. Check git diff for removed configuration options, CLI arguments, or YAML fields
    3. Look for changes to example files or reference documentation
    4. Grep the changed skill files for terms like "deprecated", "removed", "no longer"
    5. If skill documentation shrinks significantly, user documentation likely needs updates
    
    The initial "no updates needed" assessment should be treated as provisional - verify 
    by checking if any user-visible feature names appear in both the changes AND the 
    user-facing documentation.
  target: plugins/m42-sprint/CLAUDE.md
  confidence: medium
  source:
    context: "Phase-2 transcript showed initial assessment of 'no documentation subagents needed' at line 57, but workflow later spawned 3 subagents and removed 732+ lines of documentation. Suggests gap in initial assessment logic for detecting user-facing feature removal."

- id: touch-before-write-for-new-artifacts
  status: duplicate
  title: Touch-before-write pattern for new artifact files
  problem: |
    Write tool requires files to exist and be read before writing. When creating new artifact 
    files during sprint phases (like docs-summary.md), this creates a chicken-and-egg problem - 
    the file doesn't exist yet, so Write fails.
    
    A future agent might struggle with this when trying to create new summary artifacts or 
    other files in the sprint artifacts directory.
  solution: |
    Use `Bash(touch <filepath>)` to create an empty file first, then use Write tool. This 
    bypasses Write tool's safety requirement while still enabling file creation.
    
    Example from phase-2 line 202:
    ```bash
    touch artifacts/docs-summary.md
    # Then use Write tool to add content
    ```
    
    Note: This learning is already documented in root CLAUDE.md lines 25-27 under "Touch 
    command bypasses Write tool file existence requirement" and "Bash with heredoc creates 
    files without Read requirement". Marking as duplicate.
  target: /home/koni/projects/m42-claude-plugins/CLAUDE.md
  confidence: high
  source:
    context: "Phase-2 transcript line 202 shows touch command used before writing docs-summary.md. However, this pattern is already documented in root CLAUDE.md."
  - id: tooling-update-affected-plugin-first
    status: pending
    title: Tooling update starts with affected plugin identification
    problem: |
      When reviewing tooling update transcripts, the workflow begins with identifying which plugin was affected by documentation changes. The reasoning blocks show "Affected plugin: **m42-sprint**" as the first step, followed by analysis of what was modified.
    solution: |
      Tooling update workflows should follow this sequence: 1) Identify affected plugin(s) from the sprint changes, 2) Analyze what was modified in documentation/reference files, 3) Review all commands and skills for consistency, 4) Update any tooling files that still reference deprecated features, 5) Commit changes with standardized messages. This ensures systematic coverage of all artifacts that might need synchronization with documentation changes.
    target: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: extract
      context: "Phase 3 tooling update from sprint 2026-02-01_extract-command-refactor"

  - id: tooling-review-parallel-subagent-pattern
    status: pending
    title: Tooling reviews spawn parallel subagents per artifact
    problem: |
      The reasoning blocks show "All 12 commands reviewed - all **Unchanged**" and "All 4 skills reviewed - all **Unchanged**", indicating a review pattern that processes multiple artifacts. The workflow reviewed commands and skills separately but systematically to find references to deprecated features.
    solution: |
      Tooling update reviews should spawn parallel subagents (one per command, one per skill) to check for deprecated feature references. Each subagent reports "Changed" or "Unchanged" status. This enables efficient batch processing - the operator can spawn 12 command reviewers in parallel, collect results, then spawn 4 skill reviewers. The pattern scales well for plugins with many artifacts and provides clear audit trail of what was checked.
    target: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/plugins/m42-meta-toolkit/CLAUDE.md
    confidence: medium
    source:
      tool: extract
      context: "Phase 3 tooling update from sprint 2026-02-01_extract-command-refactor"

  - id: tooling-commit-messages-follow-convention
    status: pending
    title: Tooling update commits use docs/tooling scope prefix
    problem: |
      The reasoning blocks show two commits were created: "docs(creating-sprints): sync with implementation - remove depends-on feature" and "tooling: commands and skills synced". This indicates a commit message convention for tooling update phases that separates documentation sync from general tooling sync.
    solution: |
      Use two-tier commit message convention for tooling updates: 1) "docs(scope): sync with implementation - <what changed>" for documentation/reference file updates within specific skills/commands, 2) "tooling: commands and skills synced" for the general review completion commit. The docs() commits are specific to individual artifacts, while the tooling commit marks the completion of the systematic review across all artifacts.
    target: /home/koni/projects/m42-claude-plugins/trees/2026-02-01_extract-command-refactor/CLAUDE.md
    confidence: medium
    source:
      tool: extract
      context: "Phase 3 tooling update from sprint 2026-02-01_extract-command-refactor"
