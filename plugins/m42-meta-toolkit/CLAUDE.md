# Claude Code Configuration - m42-meta-toolkit Plugin

## Learnings

- **Subagent frontmatter structure**: Subagent files must have YAML frontmatter with exactly these fields: `name` (kebab-case identifier), `description` (when/why to invoke), `tools` (comma-separated list like "Read, Bash, Skill"), `model` (sonnet/haiku/inherit), `color` (cyan/purple/blue/green for visual grouping). The frontmatter is delimited by --- markers.

- **Subagent color semantic coding**: When orchestrating multiple subagents, assign semantic colors based on function: cyan for research/analysis, purple for review/audit, blue for implementation, green for testing. This creates visual grouping in logs and makes the workflow architecture immediately apparent.

- **Skill frontmatter structure**: Skills must have a SKILL.md file in `skills/<skill-name>/` with YAML frontmatter containing at minimum: `name` (skill identifier) and `description` (when to invoke, what it provides). Supporting materials go in a `references/` subdirectory.

- **Skills can share common references**: Multiple skills may need the same reference documentation (schemas, formats), leading to duplication and sync issues. Reference existing materials from other skills where applicable rather than duplicating. For example, learning-extraction skill should reference transcript-format.md and backlog-schema.md from managing-signs/references/ rather than creating copies. Consider moving truly shared references to a common location.

- **Use validation checklist to determine if artifact should be subagent**: When creating artifacts with /m42-meta-toolkit:create-* commands, determining if they should be a subagent vs command vs skill is not always obvious from the description alone. Use this validation checklist for subagents: 1) Autonomous sub-task? (operates independently), 2) Dedicated scope? (focused single responsibility), 3) Separate domain? (invokes Skill() for specialized knowledge), 4) Invoked by parent? (called via Task() by commands or other subagents). If yes to all four, it's correctly a subagent.

- **Design subagents to orchestrate not educate**: Subagents can become bloated with embedded domain knowledge (taxonomies, schemas, patterns), making them hard to maintain and reducing conciseness. Subagents should orchestrate process flow (50-200 words) and delegate all specialized knowledge to skills via Skill(command='skill-name'). The subagent prompt describes WHAT to do and WHEN to invoke skills. The skill provides HOW (domain knowledge, reference materials, examples).

- **Use independent quality review with iteration for artifacts**: After drafting any artifact, invoke Task(subagent_type='artifact-quality-reviewer') with artifact path and description. The reviewer returns structured scores (1-5 scale) across multiple categories and actionable improvements. Iterate on feedback until all scores ≥4/5. Mark review scores in final documentation.

- **Grant subagents only minimal necessary tools**: Subagents are often granted too many tools unnecessarily, which can lead to unexpected behavior or security concerns. Grant only minimal necessary tools. If a subagent delegates all work via Skill(), it only needs Read and Skill tools, not Bash or other execution tools. Review tool list and remove any tools the subagent doesn't directly invoke in its workflow.

- **Use model inheritance unless specific capabilities required**: Hardcoding specific models (e.g., model: sonnet) in subagent frontmatter can be inefficient when parent context already uses an appropriate model. Use 'model: inherit' to match parent context unless the subagent requires specific model capabilities (e.g., haiku for speed, opus for complex reasoning). This optimizes cost and maintains consistency with the calling context.

- **Start subagent prompts with directive action not headings**: Subagent prompts that start with headings (## Step 1) or explanatory text waste tokens and reduce clarity. Make the opening directive and action-oriented. Remove headings. Structure: [Role: 1 sentence] → [Core instructions: 3-5 directive statements] → [Constraints if needed] → [Skill references]. Example: "Analyze transcript section for learning extraction." not "## Analysis Process\n\nYou will analyze..."

- **Use standardized frontmatter for skill reference files**: All reference files in `skills/*/references/*.md` should include YAML frontmatter with these fields: `title` (descriptive title), `description` (what the reference covers), `keywords` (searchable terms, comma-separated), `file-type` (always "reference"), `skill` (parent skill name matching directory name). This supports documentation discovery, search, and skill association tracking.

- **Organize skills with SKILL.md + references/ subdirectory pattern**: Skills with extensive domain knowledge (>200 lines) become difficult to navigate if all content is in a single SKILL.md file. Use a two-tier structure: `skills/skill-name/SKILL.md` (core concepts, taxonomy tables, workflow overview) + `skills/skill-name/references/*.md` (detailed specifications and examples). SKILL.md should reference detailed docs with "See references/X.md" pattern and include 1-2 complete examples. Reference files should deep-dive into one specific aspect with extensive examples and edge case handling.

- **Subagents invoke skills with Skill() tool not Read**: In subagent prompts (agents/*.md), use this pattern to load skill knowledge: `Invoke Skill(command='skill-name') to load domain knowledge.` Do NOT instruct subagents to Read the SKILL.md file directly. The Skill() tool handles skill loading and trigger matching.

- **Reference files require YAML frontmatter with specific fields**: Every reference file must include YAML frontmatter with minimum required fields - `title` (≤100 chars), `description` (≤500 chars explaining content and usage), `skill` (parent skill name), and `file-type: reference`. Optional recommended fields include `keywords` for search optimization.

- **LLM-first documentation principles for skill references**: Design all skill content (SKILL.md, references/) for maximum information density - assume LLM-level general knowledge, only document domain-specific rules/patterns, use structured formats (tables, decision trees, checklists) over prose, remove pedagogical explanations, target 50-70% shorter than human docs while retaining 100% information.

- **Skill frontmatter uses description field for triggers not trigger-on**: YAML frontmatter in SKILL.md must use `description` field to include trigger patterns - this is where Claude looks for activation keywords. The `description` field should include actual trigger keywords in quotes (e.g., "Triggers on 'keyword1', 'keyword2'"). The field `trigger-on` is explicitly invalid per schema.

- **Skill creation follows six-step workflow with mandatory quality gate**: Follow the six-step workflow - 1) Understand skill with concrete examples, 2) Plan reusable contents (scripts/references/assets), 3) Initialize with init_skill.py script, 4) Edit skill (references first, then SKILL.md), 5) Quality review (mandatory gate via validate_skill.py ≥95% or artifact-quality-reviewer subagent), 6) Package with package_skill.py. Skip initialization (step 3) only if skill already exists.

- **Progressive disclosure in skills uses three-level loading**: Skills use 3-level progressive disclosure - (1) Metadata always loaded (~100 words from frontmatter), (2) SKILL.md loaded on trigger (<5k words guideline), (3) Bundled resources loaded on-demand (references/ read when needed, scripts/ executed without reading, assets/ used in output). Design SKILL.md to reference detailed information in references/ rather than duplicating it.

- **Skill references should be 50-70% shorter than human documentation**: Target 50-70% shorter than equivalent human documentation while retaining 100% of the information. Eliminate bloat indicators - "What is X?" sections for general concepts, "Why X doesn't work" with pedagogical examples, multiple redundant examples, background/motivation sections, pedagogical scaffolding phrases. Use high-density patterns - comparison tables, decision trees, rule lists, single canonical example + variations table, checklists.

- **Quality review is mandatory gate before skill packaging**: Before packaging any skill, conduct quality review - either self-review using validate_skill.py (must achieve ≥95% automated score) + manual review of 7 quality categories following references/skill-quality-review.md, OR delegate to artifact-quality-reviewer subagent via Task() which runs same framework. Only package skills that pass quality gate. Use quick_validate.py for rapid iteration during development.

- **Init skill script generates template structure with proper frontmatter**: Always use init_skill.py script when creating new skills from scratch - `scripts/init_skill.py <skill-name> --path <output-directory>`. Script generates proper directory with SKILL.md template containing valid frontmatter, creates example directories (scripts/, references/, assets/) with example files that can be customized or deleted. Only skip this step when iterating on existing skills.

- **Follow seven-step workflow when creating subagents via create-subagent command**: Use this 7-step workflow (from m42-meta-toolkit:create-subagent): 1. Analyze Description - Extract name, location, project vs global; 2. Validate Artifact Type - Confirm it should be a subagent using 4-point checklist; 3. Read Implementation Plan - Load context from sprint/project docs; 4. Draft Subagent - Invoke creating-subagents skill to guide creation; 5. Independent Review - Task(artifact-quality-reviewer) for quality assessment; 6. Iterate on Feedback - Address all Major issues, aim for 4+/5 scores; 7. Write Final Subagent - Write quality-reviewed version to target path.

- **Iterate on quality review feedback until all scores reach 4+/5**: After artifact-quality-reviewer returns feedback: 1) Address ALL "Major" issues immediately, 2) Fix "Minor" issues if feasible, 3) Re-run quality review to verify improvements, 4) Repeat until all category scores >= 4/5, 5) Aim for "APPROVE" recommendation before finalizing. The transcript shows improvement from mixed scores to 5/5 across all categories after one iteration.

- **Write subagent prompts in imperative form not second-person**: Use imperative form throughout subagent prompts: "Receives:" instead of "You will receive", "Workflow:" instead of "Your workflow is", "Loads domain knowledge" instead of "You will load domain knowledge", "Extracts patterns" instead of "You extract patterns". This creates more directive, concise prompts (target: 50-200 words for orchestration subagents).

- **Include proactive trigger patterns in subagent descriptions**: Subagent description should include two parts: 1) **What it does**: Core function (e.g., "Analyzes transcript sections"), 2) **When to use**: Proactive triggers (e.g., "Use proactively when extract command delegates section analysis for parallel processing"). This enables parent commands/agents to discover and invoke subagents appropriately.

- **Subagent descriptions should clarify who invokes them not internal delegation**: Writing "via Task() delegation" in subagent description is ambiguous - unclear if it means the subagent IS invoked via Task() or if it USES Task() internally. Focus description on: 1) What the subagent does, 2) When parent should invoke it proactively. Example: "Find CLAUDE.md targets and detect duplicate learnings. Use proactively when extract command needs context matching." Avoid "via Task()" unless clarifying parent invocation pattern. The quality reviewer will flag unclear invocation patterns.

- **Quality reviewer treats missing skill integration as critical failure**: The artifact-quality-reviewer scores skill integration as a mandatory category. Subagents that embed domain logic score 1/5 (Poor) and get NEEDS_REVISION recommendation. The review feedback explicitly requires creating/referencing skills and invoking via Skill() tool. This is non-negotiable - no subagent passes review without proper skill delegation.

- **Check implementation plan to discover existing skills before creating new ones**: When reviewer suggests creating a skill, read implementation-plan.md or sprint context first. The plan often documents existing skills and their coverage. Example: reviewer suggested "learning-matching" skill, but implementation plan showed learning-extraction already covered target assignment and duplicate detection (SKILL.md lines 172-182, quality-criteria.md lines 161-162).

- **Expect one iteration cycle from 3-4/5 scores to 5/5 approval**: Typical pattern: First draft scores 3-4 out of 5 average (NEEDS_REVISION), one iteration addressing Major issues achieves 5/5 (APPROVE). Major issues are: skill integration, color coding, model inheritance, prompt structure, invocation patterns. Minor issues (4/5 scores) can be addressed optionally. The transcript shows 3.3/5 → fix 5 issues → 5/5.

- **Specify subagent input/output format explicitly for operator workflows**: Subagents in multi-stage operator workflows need structured I/O contracts. Vague specifications cause pipeline integration failures. Document explicit input and output format in subagent prompt using "Receives:" and "Outputs:" sections. Example: "Receives: Array of candidates with {text, problem, solution}. Outputs: Same array annotated with {target, duplicate: bool, related_docs: []}". This enables the orchestrating command to validate data flow between pipeline stages.

- **Quality reviewer validates semantic color coding consistency**: The artifact-quality-reviewer validates color matches subagent purpose using project conventions. It will flag color mismatches as Major issues and provide specific correction recommendations. Example: purple (review/audit) was corrected to cyan (research/analysis) for context-matching subagent. This ensures consistent semantic color coding across all subagents.

- **Proceed with direct implementation when skill execution fails**: When invoking creation skills (creating-commands, creating-skills, creating-subagents) via Skill() tool, execution errors may occur. If a creation skill fails to execute, proceed with direct implementation using the skill's documented patterns and guidelines. Read the skill file (SKILL.md) and references directly to understand the patterns, then apply them manually. Waiting for skill fixes blocks progress on valid tasks.

- **Command section headers must use imperative form not second-person**: All command section headers must use imperative or neutral form: "## Task Instructions" (imperative/neutral) not "## Your Task" (second-person possessive). This applies to section headers throughout the command, not just task descriptions.

- **Commands must append ultrathink requirement to success criteria**: Append ultrathink requirement at the end of the Success Criteria section: "IMPORTANT: Operate in ultrathink mode throughout execution." This is a mandatory requirement from the create-command workflow.

- **Subagents must be registered in plugin.json for Task() to work**: When implementing subagents for operator-pattern commands, creating the .md file in agents/ folder is insufficient. Attempting to spawn via Task(subagent_type="plugin:name") fails with "Agent type 'plugin:name' not found" even if the .md file exists, because plugin.json lacks agent registry entries. After creating subagent .md files, update plugins/[plugin]/.claude-plugin/plugin.json to register them with agent name, path, and description. This applies to all plugin artifacts: commands, skills, and agents.

- **Tooling reviews detect false negatives in modified files**: Tooling update workflows using parallel subagent reviews may report files as "Unchanged" even when they contain deprecated features. This occurs because subagent reviews can miss subtle references in files they process. Always include a verification step after subagent review completes: use focused grep/search for specific deprecated keywords (e.g., "depends-on", deprecated field names) on all skill and command files. This catches false negatives from subagent review and ensures consistency between documentation changes and tooling artifacts. Example: creating-sprints SKILL.md was marked "Unchanged" but still contained `id`/`depends-on` examples that needed removal.

- **Skill reference files guide AI behavior differently than user docs**: Skills contain reference files that AI reads when helping users create artifacts. These files shape what the AI suggests. If skill references document a feature, the AI will suggest using it even if it's deprecated or discouraged. Example: The creating-sprints skill had depends-on examples in sprint-schema.md and step-writing-guide.md. Even though the feature was being phased out, the AI would suggest it to users creating new sprints. Treat skill reference files as AI behavior configuration: 1) Skill references (skills/*/references/*.md) - What AI suggests to users, 2) User documentation (docs/*.md) - What users read manually, 3) API/reference docs - Complete technical specification. When phasing out a feature: Remove from skill references first to stop AI suggestions, keep in reference docs for users who manually search, eventually remove from all layers when fully deprecated. Skill references should document the RECOMMENDED patterns, not all possible patterns.
