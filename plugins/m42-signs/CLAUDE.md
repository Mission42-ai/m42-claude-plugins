# Claude Code Configuration - m42-signs Plugin

## Learnings

- **Extract command token-based thresholds**: The extract command should use token-based thresholds or attempt direct read with fallback to preprocessing on token errors. Previously used line count/file size which caused incorrect threshold decisions since Claude's actual limits are token-based. When refactoring, try reading transcript directly and fallback to extract-reasoning.sh preprocessing on token errors.

- **Eight-category learning taxonomy for transcript extraction**: Use an 8-category taxonomy when extracting learnings from transcripts: 1) Architectural Patterns (component relationships, design decisions), 2) Project Conventions (naming, file organization, code style), 3) Pitfalls & Gotchas (failures, edge cases), 4) Effective Strategies (what worked, debugging techniques), 5) File Relationships (dependencies, files that change together), 6) API & Library Patterns (correct API usage), 7) Build & Test Patterns (commands, organization), 8) Domain Knowledge (business logic, terminology, constraints).

- **Reference files categorize learnings with 8-category taxonomy**: Use the 8-category learning taxonomy - (1) Architectural Patterns (system structure, design decisions), (2) Project Conventions (naming, organization, style), (3) Pitfalls & Gotchas (failures, edge cases, bugs), (4) Effective Strategies (successful approaches, debugging), (5) File Relationships (dependencies, coupled changes), (6) API & Library Patterns (correct usage, gotchas), (7) Build & Test Patterns (commands, organization, environment), (8) Domain Knowledge (business logic, terminology, constraints). Each category has specific qualifying criteria and examples.

- **Extraction patterns use linguistic signals and tool sequences**: Identify learning-worthy moments using two pattern types - (1) Linguistic signals in assistant text - discoveries ("I notice...", "Actually..."), explanations ("This works because..."), decisions ("I'll use X because..."), corrections, patterns ("The pattern here is..."). (2) Tool sequences - error→investigation→resolution chains, multi-file reads for understanding, build/test failures with fixes, iterative refinement. Successful operations can indicate learnings but require strong context.

- **Confidence scoring uses evidence-based rubric with three levels**: Use evidence-based three-level rubric - HIGH requires explicit verification (error fixed + tests pass) + clear pattern + broad applicability, MEDIUM requires good insight + reasonable evidence (investigation OR single success) + somewhat reusable, LOW is speculation/single observation/context-specific. Score using decision tree and evidence types (linguistic, tool sequence, verification, scope) with point system (≥5=HIGH, 2-4=MEDIUM, <2=LOW). Reusability affects final score (narrow scope reduces by one level).

- **Extraction workflows should handle mechanical transcripts gracefully**: Transcripts containing only verification tasks (file reads, status checks, validation operations) have minimal learning value. Design extraction subagents (like chunk-analyzer) to detect mechanical patterns, return empty results with clear rationale (e.g., learnings: [] with explanation), and avoid forcing insights from low-value content. Example: Mechanical transcript input with verification tasks should produce `learnings: []` with rationale like "mechanical verification steps", "no architectural insights", preventing backlog pollution.

- **Preprocess large transcripts before chunking for extraction**: Large transcripts (100+ lines, 200KB+) contain verbose JSON structures with full file contents and tool calls. Use preprocessing scripts before chunking: 1) transcript-summary.sh for stats (message counts, block counts, error counts), 2) extract-reasoning.sh to extract only reasoning blocks (text content) from JSONL. Then chunk the preprocessed reasoning blocks for subagent analysis. This gives subagents focused content without tool call noise or JSON structure overhead. Example: Raw 102-line/290KB transcript → 15 reasoning blocks → 8 focused learnings.

- **Validate extraction output matches backlog schema exactly**: Extraction workflows that produce learnings for backlog.yaml must generate exact schema compliance. Required fields: id (kebab-case), status (pending), title (5-10 words), problem (multi-line literal block), solution (multi-line literal block), target (absolute path), confidence (high|medium|low). Check YAML structure, field presence, value types, and formatting (pipe for multi-line) during testing to ensure output can be merged without manual reformatting.

- **Plugin.json must register commands, skills, and agents for Task() invocation**: Plugin.json files must include registry arrays to make artifacts discoverable by Claude Code's invocation mechanisms: `commands` array for slash commands, `skills` array for Skill() tool, `agents` array for Task() tool. Without proper registration, commands cannot invoke subagents via Task() and subagents cannot load skills via Skill(), breaking operator pattern workflows. The existence of .md files in agents/ or skills/ directories is not sufficient - they must be explicitly registered. Check other plugin.json files or Claude Code plugin schema for proper registry structure examples.

- **Operator pattern workflows fail completely without plugin registry**: The operator pattern separates orchestration (in commands) from domain logic (in skills) using subagent delegation. When implementing operator pattern workflows: 1) Create all artifacts (.md files in correct directories), 2) Register ALL artifacts in plugin.json before testing, 3) Verify registration with available tools/agents list, 4) Test the complete workflow end-to-end. The operator pattern has a hard dependency on the plugin registry - without registration, the entire workflow is non-functional. This is especially critical for refactoring existing commands to operator pattern, where forgetting registration makes the new implementation completely broken.

- **Existing vs new subagent registration reveals incomplete migration**: When adding new artifacts to existing plugins: 1) Check plugin.json to see what's currently registered, 2) Follow the same registration pattern for new artifacts, 3) Don't assume "plugin already works" means registration will happen automatically. Existing registered artifacts prove the plugin.json structure exists and is being read. New artifacts must be added to the same registry arrays. This is easy to miss when incrementally enhancing a plugin - the plugin.json update is a manual step that creation commands/subagents don't handle automatically.

- **Skills need registration in plugin.json for Skill() tool invocation**: Skills have two invocation methods: 1) Direct reference via @ syntax in prompts (e.g., @learning-extraction), 2) Explicit loading via Skill() tool (e.g., Skill(command='learning-extraction')). Both methods may require proper registration in plugin.json's skills array. The skill files must exist in correct directory structure AND be registered. Verify skills array in plugin.json includes all skills that subagents will invoke via Skill() tool.

- **Summary phase transcripts are mechanical with low learning extraction value**: When selecting transcripts for learning extraction: prioritize implementation phases (step-2_implement, step-3_test, step-4_refactor) over summary phases. Look for phases with debugging, error recovery, or architectural decisions. Summary/documentation phases are completion ceremonies with minimal learning content. For testing extract command itself, use mechanical transcripts to verify edge case handling ("Session appears mechanical - no learning patterns detected" output path). For actual learning extraction, target transcripts with problem-solving activity.
